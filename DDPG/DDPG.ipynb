{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TRahudJ8rMqD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-LCK-bAoJwe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# PARAMETERS\n",
        "\n",
        "env_name = \"Pendulum-v0\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "env.reset()\n",
        "\n",
        "state_size = env.observation_space.shape\n",
        "action_size = env.action_space.shape\n",
        "buffer_size = 1000 * 1000\n",
        "batch_size = 64\n",
        "action_low = env.action_space.low \n",
        "action_high = env.action_space.high \n",
        "\n",
        "discount_rate = 0.99\n",
        "actor_learning_rate = 0.0001\n",
        "critic_learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "noise_scale = 0.1\n",
        "tau = 0.001\n",
        "\n",
        "random_steps = 50000\n",
        "total_episodes = 1000 * 1000\n",
        "steps_per_episode = 1000\n",
        "\n",
        "run_num = 1\n",
        "run_dir = '/run_' + str(run_num)\n",
        "save_model_step = 10\n",
        "print_progress_step = 10\n",
        "model_dir = 'results' + run_dir\n",
        "tensorboard_dir = 'summaries' + run_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZ7gSuPz7uqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Actor:\n",
        "    def __init__(self, state, state_size, action_size, action_low, action_high, training, momentum, learning_rate, batch_size, scope):\n",
        "        self.state = state\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_low = action_low\n",
        "        self.action_high = action_high\n",
        "        self.training = training\n",
        "        self.momentum = momentum\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.scope = scope\n",
        "        \n",
        "        dense1_units = 400\n",
        "        dense2_units = 300\n",
        "        \n",
        "        with tf.variable_scope(self.scope):\n",
        "            # input\n",
        "            self.input_norm = tf.layers.batch_normalization(self.state, momentum=momentum, training=training, \n",
        "                                                            name='input_norm')\n",
        "            \n",
        "            # first layer\n",
        "            minval1 = -1/(tf.sqrt(tf.to_float(np.prod(self.state_size))))\n",
        "            maxval1 = 1/(tf.sqrt(tf.to_float(np.prod(self.state_size))))\n",
        "            \n",
        "            self.dense1_init = tf.layers.dense(self.input_norm, units=dense1_units, name='dense1',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval1, maxval1), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval1, maxval1)) \n",
        "            \n",
        "            self.dense1_norm = tf.layers.batch_normalization(self.dense1_init, momentum=momentum, training=training, \n",
        "                                                             name='dense1')\n",
        "            \n",
        "            self.dense1 = tf.nn.relu(self.dense1_norm, name='dense1')\n",
        "            \n",
        "            # second layer\n",
        "            minval2 = -1/(tf.sqrt(tf.to_float(np.prod(dense1_units))))\n",
        "            maxval2 = 1/(tf.sqrt(tf.to_float(np.prod(dense1_units))))\n",
        "            \n",
        "            self.dense2_init = tf.layers.dense(self.dense1, units=dense2_units, name='dense2',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval2, maxval2), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval2, maxval2))\n",
        "            \n",
        "            self.dense2_norm = tf.layers.batch_normalization(self.dense2_init, momentum=momentum, training=training, \n",
        "                                                             name='dense2')\n",
        "            \n",
        "            self.dense2 = tf.nn.relu(self.dense2_norm, name='dense2')\n",
        "            \n",
        "            # output\n",
        "            minval3 = -0.003\n",
        "            maxval3 = 0.003\n",
        "            \n",
        "            self.output_init = tf.layers.dense(self.dense2, units=np.prod(self.action_size), name='output',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval3, maxval3), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval3, maxval3))\n",
        "            \n",
        "            self.output_tanh = tf.nn.tanh(self.output_init, name='output')\n",
        "            \n",
        "            self.output = tf.multiply(0.5, tf.multiply(self.output_tanh, (self.action_high-self.action_low)) + (self.action_high+self.action_low))\n",
        "            \n",
        "            self.network_params = tf.trainable_variables(scope=self.scope)\n",
        "            \n",
        "    def train(self, critic_output):\n",
        "        with tf.variable_scope(self.scope):\n",
        "            with tf.variable_scope('train'):\n",
        "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.gradients = tf.gradients(self.output, self.network_params, -critic_output)\n",
        "                self.gradients_mean = list(map(lambda x: tf.divide(x, self.batch_size), self.gradients))\n",
        "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.scope)\n",
        "                with tf.control_dependencies(update_ops):\n",
        "                    train_step = self.optimizer.apply_gradients(zip(self.gradients_mean, self.network_params))\n",
        "                \n",
        "                return train_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5249H1xetgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Critic:\n",
        "    def __init__(self, state, action, state_size, action_size, action_low, action_high, training, momentum, learning_rate, batch_size, scope):\n",
        "        self.state = state\n",
        "        self.action = action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_low = action_low\n",
        "        self.action_high = action_high\n",
        "        self.training = training\n",
        "        self.momentum = momentum\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.scope = scope\n",
        "        \n",
        "        dense1_units = 400\n",
        "        dense2_units = 300\n",
        "        \n",
        "        with tf.variable_scope(self.scope):\n",
        "            # input\n",
        "            self.input_norm = tf.layers.batch_normalization(self.state, momentum=momentum, training=training, \n",
        "                                                                  name='input_norm')\n",
        "            \n",
        "            # first layer\n",
        "            minval1 = -1/(tf.sqrt(tf.to_float(np.prod(self.state_size))))\n",
        "            maxval1 = 1/(tf.sqrt(tf.to_float(np.prod(self.state_size))))\n",
        "            \n",
        "            self.dense1_init = tf.layers.dense(self.input_norm, units=dense1_units, name='dense1',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval1, maxval1), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval1, maxval1)) \n",
        "            \n",
        "            self.dense1_norm = tf.layers.batch_normalization(self.dense1_init, momentum=momentum, training=training,\n",
        "                                                             name='dense1')\n",
        "            \n",
        "            self.dense1 = tf.nn.relu(self.dense1_norm, name='dense1')\n",
        "            \n",
        "            # second layer (action appears)\n",
        "            minval2 = -1/(tf.sqrt(tf.to_float(np.prod(dense1_units + np.prod(self.action_size)))))\n",
        "            maxval2 = 1/(tf.sqrt(tf.to_float(np.prod(dense1_units + np.prod(self.action_size)))))\n",
        "            \n",
        "            self.dense2_init = tf.layers.dense(self.dense1, units=dense2_units, name='dense2_init',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval2, maxval2), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval2, maxval2))\n",
        "            \n",
        "            self.dense2_action = tf.layers.dense(self.action, units=dense2_units, name='dense2_action',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval2, maxval2), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval2, maxval2))\n",
        "            \n",
        "            self.dense2 = tf.nn.relu(self.dense2_init + self.dense2_action, name='dense2')\n",
        "            \n",
        "            #output\n",
        "            minval3 = -0.003\n",
        "            maxval3 = 0.003\n",
        "            \n",
        "            self.output = tf.layers.dense(self.dense2, units=1, name='output',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval3, maxval3), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval3, maxval3))\n",
        "            \n",
        "            self.network_params = tf.trainable_variables(scope=self.scope)\n",
        "            \n",
        "            self.critic_grads = tf.gradients(self.output, self.action)\n",
        "\n",
        "            \n",
        "    def train(self, target_q):\n",
        "        with tf.variable_scope(self.scope):\n",
        "            with tf.variable_scope('train'):\n",
        "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.loss = tf.losses.mean_squared_error(target_q, self.output)\n",
        "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.scope)\n",
        "                with tf.control_dependencies(update_ops):\n",
        "                    train_step = self.optimizer.minimize(self.loss, var_list=self.network_params)\n",
        "                \n",
        "                return train_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WI8Dgrk2GJ1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class OrnsteinUhlenbeckNoise:\n",
        "    def __init__(self, mu, sigma=0.3, theta=0.15, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'OrnsteinUhlenbeckNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqtBn4CUknbp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, state_size, action_size, buffer_size, batch_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.now_size = 0\n",
        "        self.now_pos = 0\n",
        "        \n",
        "        self.actions = np.empty((self.buffer_size,) + self.action_size, dtype=np.float32)\n",
        "        self.rewards = np.empty(self.buffer_size, dtype=np.float32)\n",
        "        self.terminals = np.empty(self.buffer_size, dtype=np.bool)\n",
        "        self.states = np.empty((self.buffer_size,) + self.state_size, dtype=np.float32)\n",
        "\n",
        "        self.batch_states = np.empty((self.batch_size,) + self.state_size, dtype=np.float32)\n",
        "        self.batch_next_state = np.empty((self.batch_size,) + self.state_size, dtype=np.float32)\n",
        "\n",
        "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
        "\n",
        "    def add(self, state, action, reward, terminal):\n",
        "        self.states[self.now_pos, ...] = state\n",
        "        self.actions[self.now_pos, ...] = action\n",
        "        self.rewards[self.now_pos] = reward\n",
        "        self.terminals[self.now_pos] = terminal\n",
        "        self.now_size = max(self.now_size, self.now_pos + 1)\n",
        "        self.now_pos = (self.now_pos + 1) % self.buffer_size\n",
        "\n",
        "    def get_indicies(self):\n",
        "        for i in range(self.batch_size):\n",
        "            while True:\n",
        "                index = np.random.randint(1, self.now_size)\n",
        "                # state and next_state must be from one episode\n",
        "                if index == self.now_pos:\n",
        "                    continue\n",
        "                # state and next_state must be from one episode\n",
        "                if self.terminals[index-1]:\n",
        "                    continue\n",
        "                break\n",
        "                \n",
        "            self.indices[i] = index\n",
        "\n",
        "    def get_batch(self):\n",
        "        self.get_indicies()\n",
        "        for i, index in enumerate(self.indices):\n",
        "            self.batch_states[i, ...] = self.states[index - 1, ...]\n",
        "            self.batch_next_state[i, ...] = self.states[index, ...]\n",
        "\n",
        "        return self.batch_states, self.actions[self.indices], self.rewards[self.indices], self.terminals[self.indices], self.batch_next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwtVd6DnT6ba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_target_network(network_params, target_network_params, tau):\n",
        "    copy = []\n",
        "    for old, new in zip(network_params, target_network_params):\n",
        "        copy.append(new.assign((tf.multiply(old, tau) + tf.multiply(new, 1. - tau))))\n",
        "\n",
        "    return copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JM7zucU8t6y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# INITIALIZATION\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# placeholders\n",
        "state_ph = tf.placeholder(dtype=tf.float32, shape=((None,) + state_size))\n",
        "action_ph = tf.placeholder(dtype=tf.float32, shape=((None,) + action_size))\n",
        "target_q_ph = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
        "critic_grads_ph = tf.placeholder(dtype=tf.float32, shape=((None,) + action_size))\n",
        "training_ph = tf.placeholder_with_default(input=True, shape=None)\n",
        "\n",
        "# init actor networks\n",
        "actor = Actor(state_ph, state_size, action_size, action_low, action_high, training_ph, momentum, actor_learning_rate, batch_size, 'actor')\n",
        "actor_target = Actor(state_ph, state_size, action_size, action_low, action_high, training_ph, momentum, actor_learning_rate, batch_size, 'actor_target')\n",
        "\n",
        "# init critic networks\n",
        "critic = Critic(state_ph, action_ph, state_size, action_size, action_low, action_high, training_ph, momentum, critic_learning_rate, batch_size, 'critic')\n",
        "critic_target = Critic(state_ph, action_ph, state_size, action_size, action_low, action_high, training_ph, momentum, critic_learning_rate, batch_size, 'critic_target')\n",
        "\n",
        "# train operations\n",
        "critic_train = critic.train(target_q_ph)\n",
        "actor_train = actor.train(critic_grads_ph)\n",
        "\n",
        "# update network parameters operations\n",
        "update_critic_target = update_target_network(critic.network_params, critic_target.network_params, tau)\n",
        "update_actor_target = update_target_network(actor.network_params, actor_target.network_params, tau)\n",
        "\n",
        "# buffer and noise\n",
        "replay_buffer = ReplayBuffer(state_size, action_size, buffer_size, batch_size)\n",
        "noise = OrnsteinUhlenbeckNoise(mu=np.zeros(action_size))\n",
        "noise_scaling = noise_scale * (action_high - action_low)\n",
        "\n",
        "# make directories\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "\n",
        "# init session, saver and writer\n",
        "sess = tf.Session()\n",
        "saver = tf.train.Saver(max_to_keep=10)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "writer = tf.summary.FileWriter(tensorboard_dir, graph=sess.graph)\n",
        "\n",
        "# copy networks to target networks\n",
        "sess.run(update_target_network(critic.network_params, critic_target.network_params, 1))\n",
        "sess.run(update_target_network(actor.network_params, actor_target.network_params, 1))\n",
        "\n",
        "# episode reward for tensorboard scalar\n",
        "episode_reward_var = tf.Variable(0.0, trainable=False)\n",
        "tf.summary.scalar('Episode_reward', episode_reward_var)\n",
        "summary_op = tf.summary.merge_all()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TRAINING\n",
        "\n",
        "# fill replay buffer with random actions\n",
        "for step in range(random_steps):\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, terminal, _ = env.step(action)\n",
        "    replay_buffer.add(state, action, reward, terminal)\n",
        "    if terminal:\n",
        "        env.reset()\n",
        "\n",
        "episode_rewards = []\n",
        "losses = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    state = env.reset()\n",
        "    noise.reset()\n",
        "    episode_reward = 0\n",
        "    episode_terminal = False\n",
        "    count_steps = 0\n",
        "\n",
        "    while not episode_terminal:\n",
        "        state_expand = np.expand_dims(state, 0)\n",
        "        action = sess.run(actor.output, feed_dict={state_ph : state_expand, training_ph : False})[0]\n",
        "        action += noise() * noise_scaling\n",
        "        state, reward, terminal, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        replay_buffer.add(state, action, reward, terminal)\n",
        "        count_steps += 1\n",
        "\n",
        "        states, actions, rewards, terminals, next_states = replay_buffer.get_batch()\n",
        "\n",
        "        # target_q = reward + discount_rate * Q'(next_state, M'(next_state))\n",
        "        next_actions = sess.run(actor_target.output, feed_dict={state_ph : next_states})\n",
        "\n",
        "        predicted_q = sess.run(critic_target.output, feed_dict={state_ph : next_states, action_ph : next_actions})[:, 0]\n",
        "\n",
        "        predicted_q[terminals] = 0\n",
        "\n",
        "        target_q = rewards + discount_rate * predicted_q\n",
        "\n",
        "        target_q_expand = np.expand_dims(target_q, 1)\n",
        "\n",
        "        # train critic\n",
        "        sess.run(critic_train, feed_dict={state_ph : states, action_ph : actions, target_q_ph : target_q_expand})\n",
        "\n",
        "        # train actor\n",
        "        actor_output = sess.run(actor.output, feed_dict={state_ph : states})\n",
        "        critic_grads = sess.run(critic.critic_grads, feed_dict={state_ph : states, action_ph : actor_output})\n",
        "        sess.run(actor_train, feed_dict={state_ph : states, critic_grads_ph : critic_grads[0]})\n",
        "\n",
        "        # update target networks\n",
        "        sess.run(update_critic_target)\n",
        "        sess.run(update_actor_target)\n",
        "\n",
        "        if terminal or count_steps == steps_per_episode:\n",
        "            episode_terminal = True\n",
        "            episode_rewards.append(episode_reward)\n",
        "            # update summary\n",
        "            summ = sess.run(summary_op, feed_dict={episode_reward_var : episode_reward})\n",
        "            writer.add_summary(summ, episode)\n",
        "           \n",
        "    \n",
        "    if episode % print_progress_step == 0:\n",
        "        # print progress\n",
        "        print('Episode =', episode, 'Mean reward =', np.mean(episode_rewards[-10:]))\n",
        "        \n",
        "    \n",
        "    if episode % save_model_step == 0:\n",
        "        saver.save(sess, model_dir, global_step=episode)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}