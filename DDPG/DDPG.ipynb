{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TRahudJ8rMqD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-LCK-bAoJwe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# PARAMETERS\n",
        "\n",
        "env_name = \"Pendulum-v0\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "env.reset()\n",
        "\n",
        "state_size = env.observation_space.shape\n",
        "action_size = env.action_space.shape\n",
        "buffer_size = 1000 * 1000\n",
        "batch_size = 64\n",
        "action_low = env.action_space.low \n",
        "action_high = env.action_space.high \n",
        "\n",
        "discount_rate = 0.99\n",
        "actor_learning_rate = 0.0001\n",
        "critic_learning_rate = 0.001\n",
        "momentum = 0.9\n",
        "noise_scale = 0.1\n",
        "tau = 0.001\n",
        "\n",
        "random_steps = 50000\n",
        "total_episodes = 1000 * 1000\n",
        "steps_per_episode = 1000\n",
        "\n",
        "run_num = 1\n",
        "run_dir = '/run_' + str(run_num)\n",
        "save_model_step = 10\n",
        "print_progress_step = 10\n",
        "model_dir = 'results' + run_dir\n",
        "tensorboard_dir = 'summaries' + run_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZ7gSuPz7uqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Actor:\n",
        "    def __init__(self, state, state_size, action_size, action_low, action_high, training, momentum, learning_rate, batch_size, scope):\n",
        "        self.state = state\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_low = action_low\n",
        "        self.action_high = action_high\n",
        "        self.training = training\n",
        "        self.momentum = momentum\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.scope = scope\n",
        "        \n",
        "        dense1_units = 400\n",
        "        dense2_units = 300\n",
        "        \n",
        "        with tf.variable_scope(self.scope):\n",
        "            # input\n",
        "            self.input_norm = tf.layers.batch_normalization(self.state, momentum=momentum, training=training, \n",
        "                                                            name='input_norm')\n",
        "            \n",
        "            # first layer\n",
        "            minval1 = -1/(tf.sqrt(tf.to_float(np.prod(self.state_size))))\n",
        "            maxval1 = 1/(tf.sqrt(tf.to_float(np.prod(self.state_size))))\n",
        "            \n",
        "            self.dense1_init = tf.layers.dense(self.input_norm, units=dense1_units, name='dense1',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval1, maxval1), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval1, maxval1)) \n",
        "            \n",
        "            self.dense1_norm = tf.layers.batch_normalization(self.dense1_init, momentum=momentum, training=training, \n",
        "                                                             name='dense1')\n",
        "            \n",
        "            self.dense1 = tf.nn.relu(self.dense1_norm, name='dense1')\n",
        "            \n",
        "            # second layer\n",
        "            minval2 = -1/(tf.sqrt(tf.to_float(np.prod(dense1_units))))\n",
        "            maxval2 = 1/(tf.sqrt(tf.to_float(np.prod(dense1_units))))\n",
        "            \n",
        "            self.dense2_init = tf.layers.dense(self.dense1, units=dense2_units, name='dense2',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval2, maxval2), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval2, maxval2))\n",
        "            \n",
        "            self.dense2_norm = tf.layers.batch_normalization(self.dense2_init, momentum=momentum, training=training, \n",
        "                                                             name='dense2')\n",
        "            \n",
        "            self.dense2 = tf.nn.relu(self.dense2_norm, name='dense2')\n",
        "            \n",
        "            # output\n",
        "            minval3 = -0.003\n",
        "            maxval3 = 0.003\n",
        "            \n",
        "            self.output_init = tf.layers.dense(self.dense2, units=np.prod(self.action_size), name='output',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval3, maxval3), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval3, maxval3))\n",
        "            \n",
        "            self.output_tanh = tf.nn.tanh(self.output_init, name='output')\n",
        "            \n",
        "            self.output = tf.multiply(0.5, tf.multiply(self.output_tanh, (self.action_high-self.action_low)) + (self.action_high+self.action_low))\n",
        "            \n",
        "            self.network_params = tf.trainable_variables(scope=self.scope)\n",
        "            \n",
        "    def train(self, critic_output):\n",
        "        with tf.variable_scope(self.scope):\n",
        "            with tf.variable_scope('train'):\n",
        "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.gradients = tf.gradients(self.output, self.network_params, -critic_output)\n",
        "                self.gradients_mean = list(map(lambda x: tf.divide(x, self.batch_size), self.gradients))\n",
        "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.scope)\n",
        "                with tf.control_dependencies(update_ops):\n",
        "                    train_step = self.optimizer.apply_gradients(zip(self.gradients_mean, self.network_params))\n",
        "                \n",
        "                return train_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5249H1xetgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Critic:\n",
        "    def __init__(self, state, action, state_size, action_size, action_low, action_high, training, momentum, learning_rate, batch_size, scope):\n",
        "        self.state = state\n",
        "        self.action = action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_low = action_low\n",
        "        self.action_high = action_high\n",
        "        self.training = training\n",
        "        self.momentum = momentum\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.scope = scope\n",
        "        \n",
        "        dense1_units = 400\n",
        "        dense2_units = 300\n",
        "        \n",
        "        with tf.variable_scope(self.scope):\n",
        "            # input\n",
        "            self.input_norm = tf.layers.batch_normalization(self.state, momentum=momentum, training=training, \n",
        "                                                                  name='input_norm')\n",
        "            \n",
        "            # first layer\n",
        "            minval1 = -1/(tf.sqrt(tf.to_float(np.prod(self.state_size))))\n",
        "            maxval1 = 1/(tf.sqrt(tf.to_float(np.prod(self.state_size))))\n",
        "            \n",
        "            self.dense1_init = tf.layers.dense(self.input_norm, units=dense1_units, name='dense1',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval1, maxval1), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval1, maxval1)) \n",
        "            \n",
        "            self.dense1_norm = tf.layers.batch_normalization(self.dense1_init, momentum=momentum, training=training,\n",
        "                                                             name='dense1')\n",
        "            \n",
        "            self.dense1 = tf.nn.relu(self.dense1_norm, name='dense1')\n",
        "            \n",
        "            # second layer (action appears)\n",
        "            minval2 = -1/(tf.sqrt(tf.to_float(np.prod(dense1_units + np.prod(self.action_size)))))\n",
        "            maxval2 = 1/(tf.sqrt(tf.to_float(np.prod(dense1_units + np.prod(self.action_size)))))\n",
        "            \n",
        "            self.dense2_init = tf.layers.dense(self.dense1, units=dense2_units, name='dense2_init',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval2, maxval2), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval2, maxval2))\n",
        "            \n",
        "            self.dense2_action = tf.layers.dense(self.action, units=dense2_units, name='dense2_action',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval2, maxval2), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval2, maxval2))\n",
        "            \n",
        "            self.dense2 = tf.nn.relu(self.dense2_init + self.dense2_action, name='dense2')\n",
        "            \n",
        "            #output\n",
        "            minval3 = -0.003\n",
        "            maxval3 = 0.003\n",
        "            \n",
        "            self.output = tf.layers.dense(self.dense2, units=1, name='output',\n",
        "                                          kernel_initializer=tf.random_uniform_initializer(minval3, maxval3), \n",
        "                                          bias_initializer=tf.random_uniform_initializer(minval3, maxval3))\n",
        "            \n",
        "            self.network_params = tf.trainable_variables(scope=self.scope)\n",
        "            \n",
        "            self.critic_grads = tf.gradients(self.output, self.action)\n",
        "\n",
        "            \n",
        "    def train(self, target_q):\n",
        "        with tf.variable_scope(self.scope):\n",
        "            with tf.variable_scope('train'):\n",
        "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.loss = tf.losses.mean_squared_error(target_q, self.output)\n",
        "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.scope)\n",
        "                with tf.control_dependencies(update_ops):\n",
        "                    train_step = self.optimizer.minimize(self.loss, var_list=self.network_params)\n",
        "                \n",
        "                return train_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WI8Dgrk2GJ1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class OrnsteinUhlenbeckNoise:\n",
        "    def __init__(self, mu, sigma=0.3, theta=0.15, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'OrnsteinUhlenbeckNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqtBn4CUknbp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, state_size, action_size, buffer_size, batch_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.now_size = 0\n",
        "        self.now_pos = 0\n",
        "        \n",
        "        self.actions = np.empty((self.buffer_size,) + self.action_size, dtype=np.float32)\n",
        "        self.rewards = np.empty(self.buffer_size, dtype=np.float32)\n",
        "        self.terminals = np.empty(self.buffer_size, dtype=np.bool)\n",
        "        self.states = np.empty((self.buffer_size,) + self.state_size, dtype=np.float32)\n",
        "\n",
        "        self.batch_states = np.empty((self.batch_size,) + self.state_size, dtype=np.float32)\n",
        "        self.batch_next_state = np.empty((self.batch_size,) + self.state_size, dtype=np.float32)\n",
        "\n",
        "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
        "\n",
        "    def add(self, state, action, reward, terminal):\n",
        "        self.states[self.now_pos, ...] = state\n",
        "        self.actions[self.now_pos, ...] = action\n",
        "        self.rewards[self.now_pos] = reward\n",
        "        self.terminals[self.now_pos] = terminal\n",
        "        self.now_size = max(self.now_size, self.now_pos + 1)\n",
        "        self.now_pos = (self.now_pos + 1) % self.buffer_size\n",
        "\n",
        "    def get_indicies(self):\n",
        "        for i in range(self.batch_size):\n",
        "            while True:\n",
        "                index = np.random.randint(1, self.now_size)\n",
        "                # state and next_state must be from one episode\n",
        "                if index == self.now_pos:\n",
        "                    continue\n",
        "                # state and next_state must be from one episode\n",
        "                if self.terminals[index-1]:\n",
        "                    continue\n",
        "                break\n",
        "                \n",
        "            self.indices[i] = index\n",
        "\n",
        "    def get_batch(self):\n",
        "        self.get_indicies()\n",
        "        for i, index in enumerate(self.indices):\n",
        "            self.batch_states[i, ...] = self.states[index - 1, ...]\n",
        "            self.batch_next_state[i, ...] = self.states[index, ...]\n",
        "\n",
        "        return self.batch_states, self.actions[self.indices], self.rewards[self.indices], self.terminals[self.indices], self.batch_next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwtVd6DnT6ba",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_target_network(network_params, target_network_params, tau):\n",
        "    copy = []\n",
        "    for old, new in zip(network_params, target_network_params):\n",
        "        copy.append(new.assign((tf.multiply(old, tau) + tf.multiply(new, 1. - tau))))\n",
        "\n",
        "    return copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JM7zucU8t6y",
        "colab_type": "code",
        "outputId": "40cdcf4f-4084-4d64-e3c6-7c56d1ed0146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18040
        }
      },
      "cell_type": "code",
      "source": [
        "# INITIALIZATION\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# placeholders\n",
        "state_ph = tf.placeholder(dtype=tf.float32, shape=((None,) + state_size))\n",
        "action_ph = tf.placeholder(dtype=tf.float32, shape=((None,) + action_size))\n",
        "target_q_ph = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
        "critic_grads_ph = tf.placeholder(dtype=tf.float32, shape=((None,) + action_size))\n",
        "training_ph = tf.placeholder_with_default(input=True, shape=None)\n",
        "\n",
        "# init actor networks\n",
        "actor = Actor(state_ph, state_size, action_size, action_low, action_high, training_ph, momentum, actor_learning_rate, batch_size, 'actor')\n",
        "actor_target = Actor(state_ph, state_size, action_size, action_low, action_high, training_ph, momentum, actor_learning_rate, batch_size, 'actor_target')\n",
        "\n",
        "# init critic networks\n",
        "critic = Critic(state_ph, action_ph, state_size, action_size, action_low, action_high, training_ph, momentum, critic_learning_rate, batch_size, 'critic')\n",
        "critic_target = Critic(state_ph, action_ph, state_size, action_size, action_low, action_high, training_ph, momentum, critic_learning_rate, batch_size, 'critic_target')\n",
        "\n",
        "# train operations\n",
        "critic_train = critic.train(target_q_ph)\n",
        "actor_train = actor.train(critic_grads_ph)\n",
        "\n",
        "# update network parameters operations\n",
        "update_critic_target = update_target_network(critic.network_params, critic_target.network_params, tau)\n",
        "update_actor_target = update_target_network(actor.network_params, actor_target.network_params, tau)\n",
        "\n",
        "# buffer and noise\n",
        "replay_buffer = ReplayBuffer(state_size, action_size, buffer_size, batch_size)\n",
        "noise = OrnsteinUhlenbeckNoise(mu=np.zeros(action_size))\n",
        "noise_scaling = noise_scale * (action_high - action_low)\n",
        "\n",
        "# make directories\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "\n",
        "# init session, saver and writer\n",
        "sess = tf.Session()\n",
        "saver = tf.train.Saver(max_to_keep=10)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "writer = tf.summary.FileWriter(tensorboard_dir, graph=sess.graph)\n",
        "\n",
        "# copy networks to target networks\n",
        "sess.run(update_target_network(critic.network_params, critic_target.network_params, 1))\n",
        "sess.run(update_target_network(actor.network_params, actor_target.network_params, 1))\n",
        "\n",
        "# episode reward for tensorboard scalar\n",
        "episode_reward_var = tf.Variable(0.0, trainable=False)\n",
        "tf.summary.scalar('Episode_reward', episode_reward_var)\n",
        "summary_op = tf.summary.merge_all()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TRAINING\n",
        "\n",
        "# fill replay buffer with random actions\n",
        "for step in range(random_steps):\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, terminal, _ = env.step(action)\n",
        "    replay_buffer.add(state, action, reward, terminal)\n",
        "    if terminal:\n",
        "        env.reset()\n",
        "\n",
        "episode_rewards = []\n",
        "losses = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    state = env.reset()\n",
        "    noise.reset()\n",
        "    episode_reward = 0\n",
        "    episode_terminal = False\n",
        "    count_steps = 0\n",
        "\n",
        "    while not episode_terminal:\n",
        "        state_expand = np.expand_dims(state, 0)\n",
        "        action = sess.run(actor.output, feed_dict={state_ph : state_expand, training_ph : False})[0]\n",
        "        action += noise() * noise_scaling\n",
        "        state, reward, terminal, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        replay_buffer.add(state, action, reward, terminal)\n",
        "        count_steps += 1\n",
        "\n",
        "        states, actions, rewards, terminals, next_states = replay_buffer.get_batch()\n",
        "\n",
        "        # target_q = reward + discount_rate * Q'(next_state, M'(next_state))\n",
        "        next_actions = sess.run(actor_target.output, feed_dict={state_ph : next_states})\n",
        "\n",
        "        predicted_q = sess.run(critic_target.output, feed_dict={state_ph : next_states, action_ph : next_actions})[:, 0]\n",
        "\n",
        "        predicted_q[terminals] = 0\n",
        "\n",
        "        target_q = rewards + discount_rate * predicted_q\n",
        "\n",
        "        target_q_expand = np.expand_dims(target_q, 1)\n",
        "\n",
        "        # train critic\n",
        "        sess.run(critic_train, feed_dict={state_ph : states, action_ph : actions, target_q_ph : target_q_expand})\n",
        "\n",
        "        # train actor\n",
        "        actor_output = sess.run(actor.output, feed_dict={state_ph : states})\n",
        "        critic_grads = sess.run(critic.critic_grads, feed_dict={state_ph : states, action_ph : actor_output})\n",
        "        sess.run(actor_train, feed_dict={state_ph : states, critic_grads_ph : critic_grads[0]})\n",
        "\n",
        "        # update target networks\n",
        "        sess.run(update_critic_target)\n",
        "        sess.run(update_actor_target)\n",
        "\n",
        "        if terminal or count_steps == steps_per_episode:\n",
        "            episode_terminal = True\n",
        "            episode_rewards.append(episode_reward)\n",
        "            # update summary\n",
        "            summ = sess.run(summary_op, feed_dict={episode_reward_var : episode_reward})\n",
        "            writer.add_summary(summ, episode)\n",
        "           \n",
        "    \n",
        "    if episode % print_progress_step == 0:\n",
        "        # print progress\n",
        "        print('Episode =', episode, 'Mean reward =', np.mean(episode_rewards[-10:]))\n",
        "        \n",
        "    \n",
        "    if episode % save_model_step == 0:\n",
        "        saver.save(sess, model_dir, global_step=episode)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-f7ad2281cb2e>:20: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-3-f7ad2281cb2e>:23: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-f7ad2281cb2e>:28: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Episode = 0 Mean reward = -1325.9862349526536\n",
            "Episode = 10 Mean reward = -1379.0853186895224\n",
            "Episode = 20 Mean reward = -1431.2582922275988\n",
            "Episode = 30 Mean reward = -1234.6649399889452\n",
            "Episode = 40 Mean reward = -1002.7581047726198\n",
            "Episode = 50 Mean reward = -653.8453193033907\n",
            "Episode = 60 Mean reward = -339.57815173665847\n",
            "Episode = 70 Mean reward = -203.5316214812877\n",
            "Episode = 80 Mean reward = -216.60701220466876\n",
            "Episode = 90 Mean reward = -172.6179050007132\n",
            "Episode = 100 Mean reward = -185.67593696918084\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Episode = 110 Mean reward = -161.10139528118188\n",
            "Episode = 120 Mean reward = -143.974057201851\n",
            "Episode = 130 Mean reward = -170.28475099871108\n",
            "Episode = 140 Mean reward = -173.99157882885103\n",
            "Episode = 150 Mean reward = -124.3354528769249\n",
            "Episode = 160 Mean reward = -164.17368643656405\n",
            "Episode = 170 Mean reward = -147.46294244344108\n",
            "Episode = 180 Mean reward = -171.68837254748092\n",
            "Episode = 190 Mean reward = -146.2630213306407\n",
            "Episode = 200 Mean reward = -155.91721901676564\n",
            "Episode = 210 Mean reward = -176.29386278318756\n",
            "Episode = 220 Mean reward = -78.99611219440304\n",
            "Episode = 230 Mean reward = -166.6381546002057\n",
            "Episode = 240 Mean reward = -147.91856859265152\n",
            "Episode = 250 Mean reward = -171.6995507106721\n",
            "Episode = 260 Mean reward = -181.6594868292656\n",
            "Episode = 270 Mean reward = -175.43895771139344\n",
            "Episode = 280 Mean reward = -200.91320767949333\n",
            "Episode = 290 Mean reward = -206.6283917864621\n",
            "Episode = 300 Mean reward = -178.98081319477083\n",
            "Episode = 310 Mean reward = -184.01002351064457\n",
            "Episode = 320 Mean reward = -242.38023733970454\n",
            "Episode = 330 Mean reward = -158.45580538576124\n",
            "Episode = 340 Mean reward = -368.6919624578712\n",
            "Episode = 350 Mean reward = -198.58677587626235\n",
            "Episode = 360 Mean reward = -221.08284859998312\n",
            "Episode = 370 Mean reward = -258.38175604245555\n",
            "Episode = 380 Mean reward = -242.57863463858538\n",
            "Episode = 390 Mean reward = -258.09297698657593\n",
            "Episode = 400 Mean reward = -260.31662449134654\n",
            "Episode = 410 Mean reward = -305.02316881344206\n",
            "Episode = 420 Mean reward = -311.4517438739141\n",
            "Episode = 430 Mean reward = -438.9682224883874\n",
            "Episode = 440 Mean reward = -391.816487487203\n",
            "Episode = 450 Mean reward = -418.77014786679865\n",
            "Episode = 460 Mean reward = -521.8569515173832\n",
            "Episode = 470 Mean reward = -440.65723327390776\n",
            "Episode = 480 Mean reward = -422.6775590939484\n",
            "Episode = 490 Mean reward = -459.25090311289523\n",
            "Episode = 500 Mean reward = -439.2412514148009\n",
            "Episode = 510 Mean reward = -423.3246667357704\n",
            "Episode = 520 Mean reward = -478.50161582321573\n",
            "Episode = 530 Mean reward = -453.7592906382605\n",
            "Episode = 540 Mean reward = -490.5182299148486\n",
            "Episode = 550 Mean reward = -495.8113607978974\n",
            "Episode = 560 Mean reward = -509.9230335672361\n",
            "Episode = 570 Mean reward = -390.0024108214692\n",
            "Episode = 580 Mean reward = -343.21750496300507\n",
            "Episode = 590 Mean reward = -471.4260707364189\n",
            "Episode = 600 Mean reward = -504.184869949509\n",
            "Episode = 610 Mean reward = -356.46877155113305\n",
            "Episode = 620 Mean reward = -346.7274663886543\n",
            "Episode = 630 Mean reward = -343.2510733205545\n",
            "Episode = 640 Mean reward = -475.87680249749735\n",
            "Episode = 650 Mean reward = -366.2787113426597\n",
            "Episode = 660 Mean reward = -346.4108132713777\n",
            "Episode = 670 Mean reward = -437.10975388343377\n",
            "Episode = 680 Mean reward = -309.3020733772065\n",
            "Episode = 690 Mean reward = -274.35958975125317\n",
            "Episode = 700 Mean reward = -297.50960616092266\n",
            "Episode = 710 Mean reward = -248.15594390297278\n",
            "Episode = 720 Mean reward = -352.5953250844885\n",
            "Episode = 730 Mean reward = -358.77116047360727\n",
            "Episode = 740 Mean reward = -332.256481795598\n",
            "Episode = 750 Mean reward = -220.50483203346212\n",
            "Episode = 760 Mean reward = -292.41963653173326\n",
            "Episode = 770 Mean reward = -263.48721815375006\n",
            "Episode = 780 Mean reward = -216.22076381107476\n",
            "Episode = 790 Mean reward = -301.58344240877267\n",
            "Episode = 800 Mean reward = -158.72356127061414\n",
            "Episode = 810 Mean reward = -78.62878200172767\n",
            "Episode = 820 Mean reward = -115.60932570850352\n",
            "Episode = 830 Mean reward = -134.4676214875255\n",
            "Episode = 840 Mean reward = -197.4289168784623\n",
            "Episode = 850 Mean reward = -144.63221464427792\n",
            "Episode = 860 Mean reward = -138.95669265416527\n",
            "Episode = 870 Mean reward = -149.1166745283079\n",
            "Episode = 880 Mean reward = -182.84208549656591\n",
            "Episode = 890 Mean reward = -145.98609569330156\n",
            "Episode = 900 Mean reward = -96.7863765485866\n",
            "Episode = 910 Mean reward = -168.68140011211113\n",
            "Episode = 920 Mean reward = -139.7450011887815\n",
            "Episode = 930 Mean reward = -193.04783007914992\n",
            "Episode = 940 Mean reward = -178.46436533585356\n",
            "Episode = 950 Mean reward = -135.49441739951484\n",
            "Episode = 960 Mean reward = -172.90388538501688\n",
            "Episode = 970 Mean reward = -222.12683861034344\n",
            "Episode = 980 Mean reward = -170.53205552256344\n",
            "Episode = 990 Mean reward = -186.36980792190155\n",
            "Episode = 1000 Mean reward = -210.48655979312238\n",
            "Episode = 1010 Mean reward = -188.5602069355491\n",
            "Episode = 1020 Mean reward = -246.4220010361198\n",
            "Episode = 1030 Mean reward = -186.2960988249694\n",
            "Episode = 1040 Mean reward = -188.51373626465758\n",
            "Episode = 1050 Mean reward = -131.01706918242402\n",
            "Episode = 1060 Mean reward = -224.95251716272327\n",
            "Episode = 1070 Mean reward = -163.34927999543748\n",
            "Episode = 1080 Mean reward = -291.80791657112553\n",
            "Episode = 1090 Mean reward = -235.1061091181162\n",
            "Episode = 1100 Mean reward = -239.63268741593888\n",
            "Episode = 1110 Mean reward = -167.98179574495722\n",
            "Episode = 1120 Mean reward = -218.95015608888167\n",
            "Episode = 1130 Mean reward = -158.06228352853367\n",
            "Episode = 1140 Mean reward = -238.80429087268686\n",
            "Episode = 1150 Mean reward = -148.8433001916677\n",
            "Episode = 1160 Mean reward = -184.37252984833938\n",
            "Episode = 1170 Mean reward = -147.5473584290417\n",
            "Episode = 1180 Mean reward = -192.5059119258779\n",
            "Episode = 1190 Mean reward = -233.80631340262806\n",
            "Episode = 1200 Mean reward = -252.7123532722591\n",
            "Episode = 1210 Mean reward = -155.54840362261012\n",
            "Episode = 1220 Mean reward = -246.69299382883258\n",
            "Episode = 1230 Mean reward = -196.52557876537546\n",
            "Episode = 1240 Mean reward = -216.1122933954835\n",
            "Episode = 1250 Mean reward = -195.9490718921938\n",
            "Episode = 1260 Mean reward = -154.91598019567874\n",
            "Episode = 1270 Mean reward = -181.26425549257797\n",
            "Episode = 1280 Mean reward = -194.79918397635484\n",
            "Episode = 1290 Mean reward = -253.17275194870643\n",
            "Episode = 1300 Mean reward = -148.7873617219716\n",
            "Episode = 1310 Mean reward = -283.9646877100191\n",
            "Episode = 1320 Mean reward = -211.23481304864595\n",
            "Episode = 1330 Mean reward = -266.30665485652213\n",
            "Episode = 1340 Mean reward = -135.94099188848153\n",
            "Episode = 1350 Mean reward = -181.5437044849738\n",
            "Episode = 1360 Mean reward = -187.8790849351725\n",
            "Episode = 1370 Mean reward = -169.0828148992889\n",
            "Episode = 1380 Mean reward = -190.75609881468822\n",
            "Episode = 1390 Mean reward = -196.5061989623186\n",
            "Episode = 1400 Mean reward = -157.39932565167777\n",
            "Episode = 1410 Mean reward = -166.9665678417942\n",
            "Episode = 1420 Mean reward = -183.63359029649848\n",
            "Episode = 1430 Mean reward = -186.8515097638089\n",
            "Episode = 1440 Mean reward = -237.71971865353325\n",
            "Episode = 1450 Mean reward = -257.90397484260126\n",
            "Episode = 1460 Mean reward = -215.66523511540072\n",
            "Episode = 1470 Mean reward = -183.7784406461717\n",
            "Episode = 1480 Mean reward = -157.03378546185814\n",
            "Episode = 1490 Mean reward = -133.78989940475495\n",
            "Episode = 1500 Mean reward = -173.4835885955996\n",
            "Episode = 1510 Mean reward = -213.416059257284\n",
            "Episode = 1520 Mean reward = -186.40596012309237\n",
            "Episode = 1530 Mean reward = -218.74075846735764\n",
            "Episode = 1540 Mean reward = -335.14075683602067\n",
            "Episode = 1550 Mean reward = -208.89926039498536\n",
            "Episode = 1560 Mean reward = -234.90010283379578\n",
            "Episode = 1570 Mean reward = -224.12822904341414\n",
            "Episode = 1580 Mean reward = -376.18466782176375\n",
            "Episode = 1590 Mean reward = -251.24608156659673\n",
            "Episode = 1600 Mean reward = -176.96733771209378\n",
            "Episode = 1610 Mean reward = -308.60253755303415\n",
            "Episode = 1620 Mean reward = -274.1942989729956\n",
            "Episode = 1630 Mean reward = -203.31987737534388\n",
            "Episode = 1640 Mean reward = -189.2061881253917\n",
            "Episode = 1650 Mean reward = -226.9838400216397\n",
            "Episode = 1660 Mean reward = -333.2947924122508\n",
            "Episode = 1670 Mean reward = -185.96188761712637\n",
            "Episode = 1680 Mean reward = -222.94007960175855\n",
            "Episode = 1690 Mean reward = -284.86508989507837\n",
            "Episode = 1700 Mean reward = -257.0342776774623\n",
            "Episode = 1710 Mean reward = -305.09576246846416\n",
            "Episode = 1720 Mean reward = -222.00656363293007\n",
            "Episode = 1730 Mean reward = -122.91642383085211\n",
            "Episode = 1740 Mean reward = -134.26153236492974\n",
            "Episode = 1750 Mean reward = -97.38475563415078\n",
            "Episode = 1760 Mean reward = -133.3989754722665\n",
            "Episode = 1770 Mean reward = -172.6232081076858\n",
            "Episode = 1780 Mean reward = -111.60432486749406\n",
            "Episode = 1790 Mean reward = -147.45494709106126\n",
            "Episode = 1800 Mean reward = -131.5482624913971\n",
            "Episode = 1810 Mean reward = -178.70828987074924\n",
            "Episode = 1820 Mean reward = -159.5948386622385\n",
            "Episode = 1830 Mean reward = -135.58527347233166\n",
            "Episode = 1840 Mean reward = -189.87216629688425\n",
            "Episode = 1850 Mean reward = -134.48753031276343\n",
            "Episode = 1860 Mean reward = -137.97731234307898\n",
            "Episode = 1870 Mean reward = -178.35359739535753\n",
            "Episode = 1880 Mean reward = -188.14717504343662\n",
            "Episode = 1890 Mean reward = -188.7166964605054\n",
            "Episode = 1900 Mean reward = -184.84910834469048\n",
            "Episode = 1910 Mean reward = -219.96352725563946\n",
            "Episode = 1920 Mean reward = -126.38579920508039\n",
            "Episode = 1930 Mean reward = -179.77695212055184\n",
            "Episode = 1940 Mean reward = -171.08756334704455\n",
            "Episode = 1950 Mean reward = -275.1459891774391\n",
            "Episode = 1960 Mean reward = -208.79714235630362\n",
            "Episode = 1970 Mean reward = -403.0580390931762\n",
            "Episode = 1980 Mean reward = -222.07313961011738\n",
            "Episode = 1990 Mean reward = -152.6523838597133\n",
            "Episode = 2000 Mean reward = -162.75763775714262\n",
            "Episode = 2010 Mean reward = -187.2295554839508\n",
            "Episode = 2020 Mean reward = -127.15644666351639\n",
            "Episode = 2030 Mean reward = -120.40814182375375\n",
            "Episode = 2040 Mean reward = -183.3700246423308\n",
            "Episode = 2050 Mean reward = -177.19795790519208\n",
            "Episode = 2060 Mean reward = -198.04806555902948\n",
            "Episode = 2070 Mean reward = -189.28958312436419\n",
            "Episode = 2080 Mean reward = -148.0412597258468\n",
            "Episode = 2090 Mean reward = -245.4299102186218\n",
            "Episode = 2100 Mean reward = -262.4055460861165\n",
            "Episode = 2110 Mean reward = -311.80319375978877\n",
            "Episode = 2120 Mean reward = -172.3147457421794\n",
            "Episode = 2130 Mean reward = -136.53912339848912\n",
            "Episode = 2140 Mean reward = -183.03526970535714\n",
            "Episode = 2150 Mean reward = -183.73201123055497\n",
            "Episode = 2160 Mean reward = -202.0074398273854\n",
            "Episode = 2170 Mean reward = -169.7469113452672\n",
            "Episode = 2180 Mean reward = -132.88042097213182\n",
            "Episode = 2190 Mean reward = -205.2878226247729\n",
            "Episode = 2200 Mean reward = -149.0689590006666\n",
            "Episode = 2210 Mean reward = -275.763596732504\n",
            "Episode = 2220 Mean reward = -178.24529494719755\n",
            "Episode = 2230 Mean reward = -174.3809531576766\n",
            "Episode = 2240 Mean reward = -186.47014596250636\n",
            "Episode = 2250 Mean reward = -158.11243995206155\n",
            "Episode = 2260 Mean reward = -232.50039361396298\n",
            "Episode = 2270 Mean reward = -253.81841866826213\n",
            "Episode = 2280 Mean reward = -253.80096780389462\n",
            "Episode = 2290 Mean reward = -251.34025866361043\n",
            "Episode = 2300 Mean reward = -160.66330866964304\n",
            "Episode = 2310 Mean reward = -140.94326680387042\n",
            "Episode = 2320 Mean reward = -129.02510107111704\n",
            "Episode = 2330 Mean reward = -188.35430908820726\n",
            "Episode = 2340 Mean reward = -376.05902719613704\n",
            "Episode = 2350 Mean reward = -206.33098583616453\n",
            "Episode = 2360 Mean reward = -347.282580844349\n",
            "Episode = 2370 Mean reward = -261.4872550321509\n",
            "Episode = 2380 Mean reward = -288.68741476584574\n",
            "Episode = 2390 Mean reward = -274.24432466664916\n",
            "Episode = 2400 Mean reward = -172.3276585926173\n",
            "Episode = 2410 Mean reward = -153.50525031726858\n",
            "Episode = 2420 Mean reward = -171.47641417397784\n",
            "Episode = 2430 Mean reward = -188.6266372344681\n",
            "Episode = 2440 Mean reward = -195.41396708121138\n",
            "Episode = 2450 Mean reward = -197.02216064244445\n",
            "Episode = 2460 Mean reward = -180.2548164528484\n",
            "Episode = 2470 Mean reward = -86.54626991751006\n",
            "Episode = 2480 Mean reward = -270.5688060267674\n",
            "Episode = 2490 Mean reward = -169.13858913182733\n",
            "Episode = 2500 Mean reward = -144.31886087181698\n",
            "Episode = 2510 Mean reward = -191.47026142593222\n",
            "Episode = 2520 Mean reward = -172.19419995280217\n",
            "Episode = 2530 Mean reward = -301.8642932900326\n",
            "Episode = 2540 Mean reward = -234.01132061007993\n",
            "Episode = 2550 Mean reward = -294.1321558743647\n",
            "Episode = 2560 Mean reward = -321.2924659259878\n",
            "Episode = 2570 Mean reward = -196.40713598384409\n",
            "Episode = 2580 Mean reward = -180.9518132208217\n",
            "Episode = 2590 Mean reward = -188.67627094736625\n",
            "Episode = 2600 Mean reward = -176.47241996395869\n",
            "Episode = 2610 Mean reward = -239.65956086813284\n",
            "Episode = 2620 Mean reward = -131.40026401637837\n",
            "Episode = 2630 Mean reward = -159.45235472561274\n",
            "Episode = 2640 Mean reward = -160.17592236075114\n",
            "Episode = 2650 Mean reward = -122.09254777705999\n",
            "Episode = 2660 Mean reward = -161.6621161887899\n",
            "Episode = 2670 Mean reward = -110.97918504240556\n",
            "Episode = 2680 Mean reward = -178.11866450974753\n",
            "Episode = 2690 Mean reward = -134.88161902221208\n",
            "Episode = 2700 Mean reward = -131.63614662378595\n",
            "Episode = 2710 Mean reward = -176.7195001800033\n",
            "Episode = 2720 Mean reward = -135.7449102271573\n",
            "Episode = 2730 Mean reward = -228.3788681571312\n",
            "Episode = 2740 Mean reward = -201.00477297219985\n",
            "Episode = 2750 Mean reward = -183.70999391789596\n",
            "Episode = 2760 Mean reward = -169.72614183863078\n",
            "Episode = 2770 Mean reward = -159.37908529745954\n",
            "Episode = 2780 Mean reward = -166.76475308075413\n",
            "Episode = 2790 Mean reward = -97.68777720493401\n",
            "Episode = 2800 Mean reward = -136.86111682747628\n",
            "Episode = 2810 Mean reward = -167.40138787729754\n",
            "Episode = 2820 Mean reward = -167.46362488739913\n",
            "Episode = 2830 Mean reward = -202.61594183864628\n",
            "Episode = 2840 Mean reward = -169.06232109640843\n",
            "Episode = 2850 Mean reward = -108.63472827080145\n",
            "Episode = 2860 Mean reward = -146.81795867180608\n",
            "Episode = 2870 Mean reward = -276.87096857148543\n",
            "Episode = 2880 Mean reward = -196.19043467502357\n",
            "Episode = 2890 Mean reward = -133.36268927655055\n",
            "Episode = 2900 Mean reward = -132.49646815558305\n",
            "Episode = 2910 Mean reward = -144.9313888497081\n",
            "Episode = 2920 Mean reward = -192.54401241898353\n",
            "Episode = 2930 Mean reward = -145.0531482976193\n",
            "Episode = 2940 Mean reward = -168.59329000865372\n",
            "Episode = 2950 Mean reward = -178.2059356103146\n",
            "Episode = 2960 Mean reward = -147.6719666944898\n",
            "Episode = 2970 Mean reward = -200.10745126575486\n",
            "Episode = 2980 Mean reward = -133.1724248252085\n",
            "Episode = 2990 Mean reward = -163.12345274141757\n",
            "Episode = 3000 Mean reward = -98.72656046095571\n",
            "Episode = 3010 Mean reward = -144.61600643048004\n",
            "Episode = 3020 Mean reward = -120.33088368623089\n",
            "Episode = 3030 Mean reward = -119.4076100097088\n",
            "Episode = 3040 Mean reward = -131.7200871859069\n",
            "Episode = 3050 Mean reward = -177.73226583527037\n",
            "Episode = 3060 Mean reward = -119.06444003827171\n",
            "Episode = 3070 Mean reward = -169.5817137582429\n",
            "Episode = 3080 Mean reward = -187.40831697551297\n",
            "Episode = 3090 Mean reward = -160.26760059792417\n",
            "Episode = 3100 Mean reward = -228.66638899890967\n",
            "Episode = 3110 Mean reward = -153.29744010527753\n",
            "Episode = 3120 Mean reward = -176.95677224274633\n",
            "Episode = 3130 Mean reward = -129.54997283336849\n",
            "Episode = 3140 Mean reward = -203.55892259259525\n",
            "Episode = 3150 Mean reward = -168.1294074896383\n",
            "Episode = 3160 Mean reward = -243.28375724378134\n",
            "Episode = 3170 Mean reward = -205.95553168088176\n",
            "Episode = 3180 Mean reward = -180.30450549296728\n",
            "Episode = 3190 Mean reward = -146.26751547689997\n",
            "Episode = 3200 Mean reward = -186.38403201500626\n",
            "Episode = 3210 Mean reward = -84.5441324667876\n",
            "Episode = 3220 Mean reward = -107.77038165367533\n",
            "Episode = 3230 Mean reward = -350.77513825358733\n",
            "Episode = 3240 Mean reward = -108.07541158239583\n",
            "Episode = 3250 Mean reward = -160.15320960269406\n",
            "Episode = 3260 Mean reward = -157.4708311463288\n",
            "Episode = 3270 Mean reward = -175.81312693228745\n",
            "Episode = 3280 Mean reward = -180.70510983061442\n",
            "Episode = 3290 Mean reward = -119.57820654141912\n",
            "Episode = 3300 Mean reward = -156.77138295430316\n",
            "Episode = 3310 Mean reward = -145.5199154211469\n",
            "Episode = 3320 Mean reward = -138.1247139675304\n",
            "Episode = 3330 Mean reward = -168.08380061846057\n",
            "Episode = 3340 Mean reward = -204.50256983702587\n",
            "Episode = 3350 Mean reward = -144.57166835321215\n",
            "Episode = 3360 Mean reward = -121.60862124574832\n",
            "Episode = 3370 Mean reward = -282.7400836094686\n",
            "Episode = 3380 Mean reward = -171.02222373048227\n",
            "Episode = 3390 Mean reward = -119.03506210429148\n",
            "Episode = 3400 Mean reward = -185.70480861507792\n",
            "Episode = 3410 Mean reward = -171.80412027208223\n",
            "Episode = 3420 Mean reward = -162.54528030996332\n",
            "Episode = 3430 Mean reward = -200.05174723367384\n",
            "Episode = 3440 Mean reward = -194.88818022979083\n",
            "Episode = 3450 Mean reward = -170.63668614724855\n",
            "Episode = 3460 Mean reward = -95.77025776439025\n",
            "Episode = 3470 Mean reward = -143.2196823897555\n",
            "Episode = 3480 Mean reward = -178.50536980834005\n",
            "Episode = 3490 Mean reward = -107.49597448649783\n",
            "Episode = 3500 Mean reward = -151.26671742301428\n",
            "Episode = 3510 Mean reward = -169.02318248694144\n",
            "Episode = 3520 Mean reward = -156.47904913496805\n",
            "Episode = 3530 Mean reward = -156.58191499480785\n",
            "Episode = 3540 Mean reward = -110.1889448257335\n",
            "Episode = 3550 Mean reward = -218.10887529570635\n",
            "Episode = 3560 Mean reward = -134.65826143745724\n",
            "Episode = 3570 Mean reward = -158.00560473196398\n",
            "Episode = 3580 Mean reward = -190.78541401029136\n",
            "Episode = 3590 Mean reward = -165.3427417237889\n",
            "Episode = 3600 Mean reward = -178.63809273547466\n",
            "Episode = 3610 Mean reward = -168.73710935078773\n",
            "Episode = 3620 Mean reward = -145.32177639385833\n",
            "Episode = 3630 Mean reward = -146.19583396069308\n",
            "Episode = 3640 Mean reward = -145.78597496835002\n",
            "Episode = 3650 Mean reward = -158.29081540543177\n",
            "Episode = 3660 Mean reward = -120.1345138684319\n",
            "Episode = 3670 Mean reward = -187.96452587335602\n",
            "Episode = 3680 Mean reward = -148.067933969053\n",
            "Episode = 3690 Mean reward = -166.91296467297337\n",
            "Episode = 3700 Mean reward = -154.04083031563724\n",
            "Episode = 3710 Mean reward = -159.38361305155723\n",
            "Episode = 3720 Mean reward = -143.7943356107611\n",
            "Episode = 3730 Mean reward = -206.07826466813373\n",
            "Episode = 3740 Mean reward = -168.2799111844808\n",
            "Episode = 3750 Mean reward = -141.84529601551466\n",
            "Episode = 3760 Mean reward = -159.12567430703623\n",
            "Episode = 3770 Mean reward = -158.81320140304788\n",
            "Episode = 3780 Mean reward = -133.7685912309339\n",
            "Episode = 3790 Mean reward = -152.87466156120706\n",
            "Episode = 3800 Mean reward = -157.86338246132658\n",
            "Episode = 3810 Mean reward = -148.20405970898787\n",
            "Episode = 3820 Mean reward = -277.9426730146517\n",
            "Episode = 3830 Mean reward = -175.23698468523278\n",
            "Episode = 3840 Mean reward = -118.73408214483584\n",
            "Episode = 3850 Mean reward = -112.526874524992\n",
            "Episode = 3860 Mean reward = -133.29197871292916\n",
            "Episode = 3870 Mean reward = -135.78374359338778\n",
            "Episode = 3880 Mean reward = -152.09388869345253\n",
            "Episode = 3890 Mean reward = -120.91026161786272\n",
            "Episode = 3900 Mean reward = -170.81958952033298\n",
            "Episode = 3910 Mean reward = -133.99873860293786\n",
            "Episode = 3920 Mean reward = -111.20364900816571\n",
            "Episode = 3930 Mean reward = -143.63291283687653\n",
            "Episode = 3940 Mean reward = -169.05992872275925\n",
            "Episode = 3950 Mean reward = -147.85721852164764\n",
            "Episode = 3960 Mean reward = -185.62388931323386\n",
            "Episode = 3970 Mean reward = -169.06868616932448\n",
            "Episode = 3980 Mean reward = -155.97448160143634\n",
            "Episode = 3990 Mean reward = -167.61855835038324\n",
            "Episode = 4000 Mean reward = -167.03553884774584\n",
            "Episode = 4010 Mean reward = -168.94700267283417\n",
            "Episode = 4020 Mean reward = -218.3261857213075\n",
            "Episode = 4030 Mean reward = -247.73114887669084\n",
            "Episode = 4040 Mean reward = -378.23812168215096\n",
            "Episode = 4050 Mean reward = -267.0914819636991\n",
            "Episode = 4060 Mean reward = -182.8894905073293\n",
            "Episode = 4070 Mean reward = -208.83983707465623\n",
            "Episode = 4080 Mean reward = -160.78888942353979\n",
            "Episode = 4090 Mean reward = -185.16048207335658\n",
            "Episode = 4100 Mean reward = -139.23021501243568\n",
            "Episode = 4110 Mean reward = -169.22148534736678\n",
            "Episode = 4120 Mean reward = -194.04523310393932\n",
            "Episode = 4130 Mean reward = -166.79366575302603\n",
            "Episode = 4140 Mean reward = -273.4299598591777\n",
            "Episode = 4150 Mean reward = -170.72282788050387\n",
            "Episode = 4160 Mean reward = -153.35865322176136\n",
            "Episode = 4170 Mean reward = -160.03579831312095\n",
            "Episode = 4180 Mean reward = -110.96324055910449\n",
            "Episode = 4190 Mean reward = -148.70350257024762\n",
            "Episode = 4200 Mean reward = -155.83850407988498\n",
            "Episode = 4210 Mean reward = -142.2403240299912\n",
            "Episode = 4220 Mean reward = -143.9197768244524\n",
            "Episode = 4230 Mean reward = -144.57412522833016\n",
            "Episode = 4240 Mean reward = -171.5492614235108\n",
            "Episode = 4250 Mean reward = -350.71013614545575\n",
            "Episode = 4260 Mean reward = -221.27155545770137\n",
            "Episode = 4270 Mean reward = -179.60288398864094\n",
            "Episode = 4280 Mean reward = -259.990623708629\n",
            "Episode = 4290 Mean reward = -276.3560807750479\n",
            "Episode = 4300 Mean reward = -272.0917914470613\n",
            "Episode = 4310 Mean reward = -205.57990368831116\n",
            "Episode = 4320 Mean reward = -173.39880192847022\n",
            "Episode = 4330 Mean reward = -205.35665756899806\n",
            "Episode = 4340 Mean reward = -157.11565784201153\n",
            "Episode = 4350 Mean reward = -133.66548342025746\n",
            "Episode = 4360 Mean reward = -122.12224792706687\n",
            "Episode = 4370 Mean reward = -123.28929609114303\n",
            "Episode = 4380 Mean reward = -154.38851207578472\n",
            "Episode = 4390 Mean reward = -144.21222893398595\n",
            "Episode = 4400 Mean reward = -175.03553293286905\n",
            "Episode = 4410 Mean reward = -122.9332628485881\n",
            "Episode = 4420 Mean reward = -100.57095000811518\n",
            "Episode = 4430 Mean reward = -169.12576165005788\n",
            "Episode = 4440 Mean reward = -180.76790385992928\n",
            "Episode = 4450 Mean reward = -145.12533426122314\n",
            "Episode = 4460 Mean reward = -187.79526796613672\n",
            "Episode = 4470 Mean reward = -145.08754642028987\n",
            "Episode = 4480 Mean reward = -141.29793505844728\n",
            "Episode = 4490 Mean reward = -147.35417924022448\n",
            "Episode = 4500 Mean reward = -159.65143002107487\n",
            "Episode = 4510 Mean reward = -131.74596933086707\n",
            "Episode = 4520 Mean reward = -145.95828080467246\n",
            "Episode = 4530 Mean reward = -169.76260878104512\n",
            "Episode = 4540 Mean reward = -122.21935234454554\n",
            "Episode = 4550 Mean reward = -201.3690004239566\n",
            "Episode = 4560 Mean reward = -215.30344954653486\n",
            "Episode = 4570 Mean reward = -121.19222342738742\n",
            "Episode = 4580 Mean reward = -156.5400662987222\n",
            "Episode = 4590 Mean reward = -214.87072801398713\n",
            "Episode = 4600 Mean reward = -178.99873012955206\n",
            "Episode = 4610 Mean reward = -149.97409343009377\n",
            "Episode = 4620 Mean reward = -177.78360501890808\n",
            "Episode = 4630 Mean reward = -112.87880220389093\n",
            "Episode = 4640 Mean reward = -125.43907461216956\n",
            "Episode = 4650 Mean reward = -147.69102999649883\n",
            "Episode = 4660 Mean reward = -191.68909773605566\n",
            "Episode = 4670 Mean reward = -160.06893960824127\n",
            "Episode = 4680 Mean reward = -146.68065832028088\n",
            "Episode = 4690 Mean reward = -158.53496094738946\n",
            "Episode = 4700 Mean reward = -145.45199152077365\n",
            "Episode = 4710 Mean reward = -155.89378887565408\n",
            "Episode = 4720 Mean reward = -146.99099386028178\n",
            "Episode = 4730 Mean reward = -169.14886103118596\n",
            "Episode = 4740 Mean reward = -151.7969369099092\n",
            "Episode = 4750 Mean reward = -124.74083754343981\n",
            "Episode = 4760 Mean reward = -160.99753537768567\n",
            "Episode = 4770 Mean reward = -166.32968016370046\n",
            "Episode = 4780 Mean reward = -123.39300548677446\n",
            "Episode = 4790 Mean reward = -134.9892911131068\n",
            "Episode = 4800 Mean reward = -133.12022829746869\n",
            "Episode = 4810 Mean reward = -157.29371631982855\n",
            "Episode = 4820 Mean reward = -209.02055696457668\n",
            "Episode = 4830 Mean reward = -124.23513888523591\n",
            "Episode = 4840 Mean reward = -214.5040543138128\n",
            "Episode = 4850 Mean reward = -220.09347060875257\n",
            "Episode = 4860 Mean reward = -180.3417557207496\n",
            "Episode = 4870 Mean reward = -122.91697179375569\n",
            "Episode = 4880 Mean reward = -148.9558715688807\n",
            "Episode = 4890 Mean reward = -204.38946093667855\n",
            "Episode = 4900 Mean reward = -158.51913661762785\n",
            "Episode = 4910 Mean reward = -111.35448736903565\n",
            "Episode = 4920 Mean reward = -158.33849764815952\n",
            "Episode = 4930 Mean reward = -182.17627707826443\n",
            "Episode = 4940 Mean reward = -157.03878578220804\n",
            "Episode = 4950 Mean reward = -148.0521984165043\n",
            "Episode = 4960 Mean reward = -309.98397459961996\n",
            "Episode = 4970 Mean reward = -122.52691042603291\n",
            "Episode = 4980 Mean reward = -178.37092527331149\n",
            "Episode = 4990 Mean reward = -157.2380551663154\n",
            "Episode = 5000 Mean reward = -118.28252569030853\n",
            "Episode = 5010 Mean reward = -175.5654522935659\n",
            "Episode = 5020 Mean reward = -203.19773888097365\n",
            "Episode = 5030 Mean reward = -177.78318603937072\n",
            "Episode = 5040 Mean reward = -173.10927724417417\n",
            "Episode = 5050 Mean reward = -141.84609355330844\n",
            "Episode = 5060 Mean reward = -192.16750793909267\n",
            "Episode = 5070 Mean reward = -186.48464585458004\n",
            "Episode = 5080 Mean reward = -235.37007650668403\n",
            "Episode = 5090 Mean reward = -175.73577862683618\n",
            "Episode = 5100 Mean reward = -168.82802689049586\n",
            "Episode = 5110 Mean reward = -132.58876928792878\n",
            "Episode = 5120 Mean reward = -165.33960154031428\n",
            "Episode = 5130 Mean reward = -142.7126920146205\n",
            "Episode = 5140 Mean reward = -281.35790327982943\n",
            "Episode = 5150 Mean reward = -154.6917840278939\n",
            "Episode = 5160 Mean reward = -167.24417352868824\n",
            "Episode = 5170 Mean reward = -95.74884566777999\n",
            "Episode = 5180 Mean reward = -165.28719521036447\n",
            "Episode = 5190 Mean reward = -183.75123497148135\n",
            "Episode = 5200 Mean reward = -145.32970686376814\n",
            "Episode = 5210 Mean reward = -84.88522819355997\n",
            "Episode = 5220 Mean reward = -118.02361311116206\n",
            "Episode = 5230 Mean reward = -104.71059516019673\n",
            "Episode = 5240 Mean reward = -143.21019030474508\n",
            "Episode = 5250 Mean reward = -147.54322325928348\n",
            "Episode = 5260 Mean reward = -142.96939741592377\n",
            "Episode = 5270 Mean reward = -132.7718270933762\n",
            "Episode = 5280 Mean reward = -122.53130289933692\n",
            "Episode = 5290 Mean reward = -146.61218018606894\n",
            "Episode = 5300 Mean reward = -136.26314230971786\n",
            "Episode = 5310 Mean reward = -121.30234668880708\n",
            "Episode = 5320 Mean reward = -214.96953284362286\n",
            "Episode = 5330 Mean reward = -139.50790535418045\n",
            "Episode = 5340 Mean reward = -108.81844536529422\n",
            "Episode = 5350 Mean reward = -149.43163757083764\n",
            "Episode = 5360 Mean reward = -105.98494461536475\n",
            "Episode = 5370 Mean reward = -224.52559507006535\n",
            "Episode = 5380 Mean reward = -121.1220359458056\n",
            "Episode = 5390 Mean reward = -252.82441497437944\n",
            "Episode = 5400 Mean reward = -176.9557320464913\n",
            "Episode = 5410 Mean reward = -147.78963894332588\n",
            "Episode = 5420 Mean reward = -124.55983024798601\n",
            "Episode = 5430 Mean reward = -149.5935658925444\n",
            "Episode = 5440 Mean reward = -146.40909734450867\n",
            "Episode = 5450 Mean reward = -168.0509736606198\n",
            "Episode = 5460 Mean reward = -158.24123577271695\n",
            "Episode = 5470 Mean reward = -299.0223957167519\n",
            "Episode = 5480 Mean reward = -249.59504143094546\n",
            "Episode = 5490 Mean reward = -122.57058006874584\n",
            "Episode = 5500 Mean reward = -119.11754733305486\n",
            "Episode = 5510 Mean reward = -406.38742864397864\n",
            "Episode = 5520 Mean reward = -185.93334895365973\n",
            "Episode = 5530 Mean reward = -164.64344095855887\n",
            "Episode = 5540 Mean reward = -158.18962456580877\n",
            "Episode = 5550 Mean reward = -168.1472444141069\n",
            "Episode = 5560 Mean reward = -160.53431210943634\n",
            "Episode = 5570 Mean reward = -203.520404395928\n",
            "Episode = 5580 Mean reward = -261.74999611670495\n",
            "Episode = 5590 Mean reward = -143.92353406998876\n",
            "Episode = 5600 Mean reward = -145.91598664621137\n",
            "Episode = 5610 Mean reward = -156.48456815662692\n",
            "Episode = 5620 Mean reward = -160.35891285712785\n",
            "Episode = 5630 Mean reward = -156.6961058729514\n",
            "Episode = 5640 Mean reward = -109.76067855501196\n",
            "Episode = 5650 Mean reward = -121.02741209599908\n",
            "Episode = 5660 Mean reward = -134.85162728319892\n",
            "Episode = 5670 Mean reward = -170.7593647607477\n",
            "Episode = 5680 Mean reward = -147.00459495520107\n",
            "Episode = 5690 Mean reward = -187.1498990590546\n",
            "Episode = 5700 Mean reward = -160.16042998595736\n",
            "Episode = 5710 Mean reward = -140.0252916494199\n",
            "Episode = 5720 Mean reward = -146.4790840371782\n",
            "Episode = 5730 Mean reward = -143.16386732959748\n",
            "Episode = 5740 Mean reward = -108.76217050411417\n",
            "Episode = 5750 Mean reward = -157.76987838468762\n",
            "Episode = 5760 Mean reward = -192.64356255826118\n",
            "Episode = 5770 Mean reward = -263.52095797395503\n",
            "Episode = 5780 Mean reward = -96.7120059325352\n",
            "Episode = 5790 Mean reward = -163.22651219397346\n",
            "Episode = 5800 Mean reward = -183.42879045020769\n",
            "Episode = 5810 Mean reward = -230.103829876284\n",
            "Episode = 5820 Mean reward = -182.34225752221823\n",
            "Episode = 5830 Mean reward = -181.19320653989757\n",
            "Episode = 5840 Mean reward = -194.32671096013203\n",
            "Episode = 5850 Mean reward = -129.891240836907\n",
            "Episode = 5860 Mean reward = -158.2104583687763\n",
            "Episode = 5870 Mean reward = -181.42907278958901\n",
            "Episode = 5880 Mean reward = -183.87484357208638\n",
            "Episode = 5890 Mean reward = -133.72717531111095\n",
            "Episode = 5900 Mean reward = -214.37133837345718\n",
            "Episode = 5910 Mean reward = -149.54731181663325\n",
            "Episode = 5920 Mean reward = -221.0168464673098\n",
            "Episode = 5930 Mean reward = -130.88137634482442\n",
            "Episode = 5940 Mean reward = -154.31316332330715\n",
            "Episode = 5950 Mean reward = -181.2966182622302\n",
            "Episode = 5960 Mean reward = -179.02236053558357\n",
            "Episode = 5970 Mean reward = -121.4361726828949\n",
            "Episode = 5980 Mean reward = -189.57417225156553\n",
            "Episode = 5990 Mean reward = -157.39724828590505\n",
            "Episode = 6000 Mean reward = -193.20950639281142\n",
            "Episode = 6010 Mean reward = -130.5361025147554\n",
            "Episode = 6020 Mean reward = -156.2820600794692\n",
            "Episode = 6030 Mean reward = -161.780935614668\n",
            "Episode = 6040 Mean reward = -143.24845011364488\n",
            "Episode = 6050 Mean reward = -86.37871992627258\n",
            "Episode = 6060 Mean reward = -132.22877193129756\n",
            "Episode = 6070 Mean reward = -201.5702772076546\n",
            "Episode = 6080 Mean reward = -211.29156114946682\n",
            "Episode = 6090 Mean reward = -155.12369127251426\n",
            "Episode = 6100 Mean reward = -205.22849997467142\n",
            "Episode = 6110 Mean reward = -179.55484655940114\n",
            "Episode = 6120 Mean reward = -157.19344824091212\n",
            "Episode = 6130 Mean reward = -215.33696734027444\n",
            "Episode = 6140 Mean reward = -165.24508885187734\n",
            "Episode = 6150 Mean reward = -205.476837438969\n",
            "Episode = 6160 Mean reward = -208.8431877262047\n",
            "Episode = 6170 Mean reward = -180.6209745580755\n",
            "Episode = 6180 Mean reward = -94.50503986056567\n",
            "Episode = 6190 Mean reward = -224.5453137576763\n",
            "Episode = 6200 Mean reward = -270.4140002318952\n",
            "Episode = 6210 Mean reward = -346.77736075523563\n",
            "Episode = 6220 Mean reward = -179.95107401390823\n",
            "Episode = 6230 Mean reward = -338.32584001545763\n",
            "Episode = 6240 Mean reward = -195.51469010202777\n",
            "Episode = 6250 Mean reward = -180.5440803332536\n",
            "Episode = 6260 Mean reward = -150.35752608831962\n",
            "Episode = 6270 Mean reward = -336.38427208696703\n",
            "Episode = 6280 Mean reward = -174.29322721086922\n",
            "Episode = 6290 Mean reward = -221.53661254655685\n",
            "Episode = 6300 Mean reward = -128.4866608077911\n",
            "Episode = 6310 Mean reward = -170.09078156108495\n",
            "Episode = 6320 Mean reward = -157.93956596849952\n",
            "Episode = 6330 Mean reward = -181.91186021068404\n",
            "Episode = 6340 Mean reward = -190.6559644668949\n",
            "Episode = 6350 Mean reward = -139.13108858064422\n",
            "Episode = 6360 Mean reward = -170.37529233325327\n",
            "Episode = 6370 Mean reward = -176.39267966444285\n",
            "Episode = 6380 Mean reward = -182.82979235678258\n",
            "Episode = 6390 Mean reward = -122.21250318240308\n",
            "Episode = 6400 Mean reward = -141.49841879146862\n",
            "Episode = 6410 Mean reward = -214.45659157586752\n",
            "Episode = 6420 Mean reward = -156.96005242679578\n",
            "Episode = 6430 Mean reward = -157.81969131676996\n",
            "Episode = 6440 Mean reward = -153.37590026300998\n",
            "Episode = 6450 Mean reward = -164.41946278383182\n",
            "Episode = 6460 Mean reward = -179.26819042627523\n",
            "Episode = 6470 Mean reward = -154.97787862851644\n",
            "Episode = 6480 Mean reward = -169.999046751633\n",
            "Episode = 6490 Mean reward = -110.67806456596881\n",
            "Episode = 6500 Mean reward = -188.7411646271367\n",
            "Episode = 6510 Mean reward = -203.50185971131452\n",
            "Episode = 6520 Mean reward = -168.9826635620809\n",
            "Episode = 6530 Mean reward = -139.94983455883423\n",
            "Episode = 6540 Mean reward = -146.5359678043511\n",
            "Episode = 6550 Mean reward = -158.43771407827677\n",
            "Episode = 6560 Mean reward = -290.1081945933922\n",
            "Episode = 6570 Mean reward = -167.25425997594442\n",
            "Episode = 6580 Mean reward = -136.59734704159942\n",
            "Episode = 6590 Mean reward = -242.9957034558136\n",
            "Episode = 6600 Mean reward = -188.28865980761842\n",
            "Episode = 6610 Mean reward = -169.29480908693102\n",
            "Episode = 6620 Mean reward = -149.26767925801275\n",
            "Episode = 6630 Mean reward = -132.2783809127121\n",
            "Episode = 6640 Mean reward = -161.10990220057698\n",
            "Episode = 6650 Mean reward = -121.35430055810912\n",
            "Episode = 6660 Mean reward = -272.63387849721414\n",
            "Episode = 6670 Mean reward = -202.31394586583406\n",
            "Episode = 6680 Mean reward = -189.40396864550397\n",
            "Episode = 6690 Mean reward = -171.62832005475676\n",
            "Episode = 6700 Mean reward = -144.78534936045637\n",
            "Episode = 6710 Mean reward = -206.30942788181392\n",
            "Episode = 6720 Mean reward = -165.81870651708428\n",
            "Episode = 6730 Mean reward = -330.5636729962793\n",
            "Episode = 6740 Mean reward = -171.34183882943876\n",
            "Episode = 6750 Mean reward = -333.9023171263459\n",
            "Episode = 6760 Mean reward = -254.8875980605381\n",
            "Episode = 6770 Mean reward = -165.67705875755766\n",
            "Episode = 6780 Mean reward = -185.0714061446004\n",
            "Episode = 6790 Mean reward = -99.96518554149505\n",
            "Episode = 6800 Mean reward = -196.16832998540946\n",
            "Episode = 6810 Mean reward = -192.9155919151347\n",
            "Episode = 6820 Mean reward = -190.36455483981936\n",
            "Episode = 6830 Mean reward = -136.88090096846793\n",
            "Episode = 6840 Mean reward = -217.91976198711365\n",
            "Episode = 6850 Mean reward = -206.0279708817965\n",
            "Episode = 6860 Mean reward = -177.50663096217858\n",
            "Episode = 6870 Mean reward = -203.37621120138456\n",
            "Episode = 6880 Mean reward = -162.1080165545146\n",
            "Episode = 6890 Mean reward = -151.48308549939856\n",
            "Episode = 6900 Mean reward = -182.65160328779606\n",
            "Episode = 6910 Mean reward = -215.2951044657594\n",
            "Episode = 6920 Mean reward = -102.67710509437737\n",
            "Episode = 6930 Mean reward = -162.46864870160175\n",
            "Episode = 6940 Mean reward = -160.68643142304478\n",
            "Episode = 6950 Mean reward = -157.9610868469709\n",
            "Episode = 6960 Mean reward = -170.3406391337474\n",
            "Episode = 6970 Mean reward = -156.18924677856995\n",
            "Episode = 6980 Mean reward = -165.87208768515663\n",
            "Episode = 6990 Mean reward = -157.40906092992907\n",
            "Episode = 7000 Mean reward = -190.71995842912992\n",
            "Episode = 7010 Mean reward = -273.43069077519857\n",
            "Episode = 7020 Mean reward = -235.27416649966295\n",
            "Episode = 7030 Mean reward = -183.2410826477719\n",
            "Episode = 7040 Mean reward = -143.4513637018542\n",
            "Episode = 7050 Mean reward = -202.18299158801207\n",
            "Episode = 7060 Mean reward = -195.80178043916203\n",
            "Episode = 7070 Mean reward = -194.45754333016856\n",
            "Episode = 7080 Mean reward = -220.25464922202713\n",
            "Episode = 7090 Mean reward = -180.92199711505373\n",
            "Episode = 7100 Mean reward = -153.34792957568052\n",
            "Episode = 7110 Mean reward = -132.90869916303103\n",
            "Episode = 7120 Mean reward = -140.30324568700226\n",
            "Episode = 7130 Mean reward = -165.73281671145546\n",
            "Episode = 7140 Mean reward = -123.599285506354\n",
            "Episode = 7150 Mean reward = -165.05601756093233\n",
            "Episode = 7160 Mean reward = -147.8505359439555\n",
            "Episode = 7170 Mean reward = -149.74915968192767\n",
            "Episode = 7180 Mean reward = -207.77905775794557\n",
            "Episode = 7190 Mean reward = -145.2810750925079\n",
            "Episode = 7200 Mean reward = -174.80375214356167\n",
            "Episode = 7210 Mean reward = -155.40668373294804\n",
            "Episode = 7220 Mean reward = -132.47973543660248\n",
            "Episode = 7230 Mean reward = -136.33555026915073\n",
            "Episode = 7240 Mean reward = -147.57681259586923\n",
            "Episode = 7250 Mean reward = -194.48211016412478\n",
            "Episode = 7260 Mean reward = -175.33694886971938\n",
            "Episode = 7270 Mean reward = -146.20070747283856\n",
            "Episode = 7280 Mean reward = -149.66328448985993\n",
            "Episode = 7290 Mean reward = -145.52384885894213\n",
            "Episode = 7300 Mean reward = -164.04555059704538\n",
            "Episode = 7310 Mean reward = -127.73539752166593\n",
            "Episode = 7320 Mean reward = -120.34656806868884\n",
            "Episode = 7330 Mean reward = -155.51196383224314\n",
            "Episode = 7340 Mean reward = -138.76155950520766\n",
            "Episode = 7350 Mean reward = -148.77490415048186\n",
            "Episode = 7360 Mean reward = -136.1403382388457\n",
            "Episode = 7370 Mean reward = -99.70176506845463\n",
            "Episode = 7380 Mean reward = -121.49181035841525\n",
            "Episode = 7390 Mean reward = -151.09362639252413\n",
            "Episode = 7400 Mean reward = -189.55911719021475\n",
            "Episode = 7410 Mean reward = -168.05783986528888\n",
            "Episode = 7420 Mean reward = -343.0927173740282\n",
            "Episode = 7430 Mean reward = -189.44338121868697\n",
            "Episode = 7440 Mean reward = -215.24281876189065\n",
            "Episode = 7450 Mean reward = -125.0042074946789\n",
            "Episode = 7460 Mean reward = -156.70382357888525\n",
            "Episode = 7470 Mean reward = -147.38672855172973\n",
            "Episode = 7480 Mean reward = -188.55724884678062\n",
            "Episode = 7490 Mean reward = -171.59723532524643\n",
            "Episode = 7500 Mean reward = -164.81667812621515\n",
            "Episode = 7510 Mean reward = -120.98565515687899\n",
            "Episode = 7520 Mean reward = -157.2854360130973\n",
            "Episode = 7530 Mean reward = -197.5218920789809\n",
            "Episode = 7540 Mean reward = -123.34364058014287\n",
            "Episode = 7550 Mean reward = -142.82827790989828\n",
            "Episode = 7560 Mean reward = -112.95584788122149\n",
            "Episode = 7570 Mean reward = -148.01030412685745\n",
            "Episode = 7580 Mean reward = -169.0952845988422\n",
            "Episode = 7590 Mean reward = -207.3871472713685\n",
            "Episode = 7600 Mean reward = -188.32825576070255\n",
            "Episode = 7610 Mean reward = -180.48306735804692\n",
            "Episode = 7620 Mean reward = -130.8569724963455\n",
            "Episode = 7630 Mean reward = -148.00830271495983\n",
            "Episode = 7640 Mean reward = -172.32706595928704\n",
            "Episode = 7650 Mean reward = -144.83359381895082\n",
            "Episode = 7660 Mean reward = -244.4778245934001\n",
            "Episode = 7670 Mean reward = -109.59444462671794\n",
            "Episode = 7680 Mean reward = -122.384684877691\n",
            "Episode = 7690 Mean reward = -134.50227101481084\n",
            "Episode = 7700 Mean reward = -159.16198443786297\n",
            "Episode = 7710 Mean reward = -143.59587128100253\n",
            "Episode = 7720 Mean reward = -142.48212224396272\n",
            "Episode = 7730 Mean reward = -113.30672895501\n",
            "Episode = 7740 Mean reward = -177.62005806008284\n",
            "Episode = 7750 Mean reward = -107.71500602176172\n",
            "Episode = 7760 Mean reward = -155.39343891503628\n",
            "Episode = 7770 Mean reward = -111.79515829689633\n",
            "Episode = 7780 Mean reward = -154.30559770106345\n",
            "Episode = 7790 Mean reward = -141.50853093106417\n",
            "Episode = 7800 Mean reward = -151.9248625066319\n",
            "Episode = 7810 Mean reward = -200.69944889273646\n",
            "Episode = 7820 Mean reward = -149.85450758634008\n",
            "Episode = 7830 Mean reward = -128.46484229900278\n",
            "Episode = 7840 Mean reward = -175.50451094664425\n",
            "Episode = 7850 Mean reward = -179.52552504384303\n",
            "Episode = 7860 Mean reward = -184.63591553845595\n",
            "Episode = 7870 Mean reward = -207.70264808310375\n",
            "Episode = 7880 Mean reward = -194.71037019436773\n",
            "Episode = 7890 Mean reward = -176.1194261358442\n",
            "Episode = 7900 Mean reward = -142.41887538810752\n",
            "Episode = 7910 Mean reward = -138.9474582161049\n",
            "Episode = 7920 Mean reward = -131.86642848571736\n",
            "Episode = 7930 Mean reward = -228.8524947302034\n",
            "Episode = 7940 Mean reward = -146.48866197281285\n",
            "Episode = 7950 Mean reward = -137.34443844528488\n",
            "Episode = 7960 Mean reward = -148.99353120904664\n",
            "Episode = 7970 Mean reward = -176.30078303224886\n",
            "Episode = 7980 Mean reward = -163.31119806615274\n",
            "Episode = 7990 Mean reward = -157.64745423817584\n",
            "Episode = 8000 Mean reward = -163.96811214569635\n",
            "Episode = 8010 Mean reward = -166.49973368202046\n",
            "Episode = 8020 Mean reward = -168.8477229155863\n",
            "Episode = 8030 Mean reward = -257.41798676584966\n",
            "Episode = 8040 Mean reward = -166.36492085148816\n",
            "Episode = 8050 Mean reward = -258.7621990468448\n",
            "Episode = 8060 Mean reward = -223.58282628207888\n",
            "Episode = 8070 Mean reward = -219.11668931520725\n",
            "Episode = 8080 Mean reward = -139.40248116803969\n",
            "Episode = 8090 Mean reward = -228.0227770060334\n",
            "Episode = 8100 Mean reward = -113.58010087914526\n",
            "Episode = 8110 Mean reward = -180.69495848218145\n",
            "Episode = 8120 Mean reward = -250.03497203289515\n",
            "Episode = 8130 Mean reward = -196.65593254149059\n",
            "Episode = 8140 Mean reward = -117.56091760613177\n",
            "Episode = 8150 Mean reward = -197.25093344003298\n",
            "Episode = 8160 Mean reward = -141.41164608201407\n",
            "Episode = 8170 Mean reward = -155.78335519242503\n",
            "Episode = 8180 Mean reward = -187.03126562979426\n",
            "Episode = 8190 Mean reward = -145.05482409925384\n",
            "Episode = 8200 Mean reward = -170.63023779686324\n",
            "Episode = 8210 Mean reward = -191.56881489889963\n",
            "Episode = 8220 Mean reward = -159.05185462346475\n",
            "Episode = 8230 Mean reward = -163.62700746130267\n",
            "Episode = 8240 Mean reward = -160.4827484528784\n",
            "Episode = 8250 Mean reward = -151.55742298047315\n",
            "Episode = 8260 Mean reward = -143.6691743932911\n",
            "Episode = 8270 Mean reward = -115.37890616345115\n",
            "Episode = 8280 Mean reward = -111.86699676325232\n",
            "Episode = 8290 Mean reward = -158.70046642476078\n",
            "Episode = 8300 Mean reward = -153.50731766381236\n",
            "Episode = 8310 Mean reward = -131.4321741591314\n",
            "Episode = 8320 Mean reward = -153.39445302646556\n",
            "Episode = 8330 Mean reward = -206.44686145575798\n",
            "Episode = 8340 Mean reward = -152.9436567121545\n",
            "Episode = 8350 Mean reward = -120.07142091122762\n",
            "Episode = 8360 Mean reward = -143.8368104831034\n",
            "Episode = 8370 Mean reward = -135.04849426046184\n",
            "Episode = 8380 Mean reward = -213.82149897674063\n",
            "Episode = 8390 Mean reward = -112.79959497237876\n",
            "Episode = 8400 Mean reward = -169.5676784527765\n",
            "Episode = 8410 Mean reward = -138.29026431368746\n",
            "Episode = 8420 Mean reward = -124.98227943699922\n",
            "Episode = 8430 Mean reward = -122.90937616417841\n",
            "Episode = 8440 Mean reward = -146.68802569060722\n",
            "Episode = 8450 Mean reward = -133.8733205640228\n",
            "Episode = 8460 Mean reward = -138.37336143439992\n",
            "Episode = 8470 Mean reward = -90.5389784078415\n",
            "Episode = 8480 Mean reward = -190.2327854730582\n",
            "Episode = 8490 Mean reward = -200.6857154894758\n",
            "Episode = 8500 Mean reward = -177.38339080154492\n",
            "Episode = 8510 Mean reward = -180.5266234028799\n",
            "Episode = 8520 Mean reward = -157.43144819411208\n",
            "Episode = 8530 Mean reward = -228.94268357981642\n",
            "Episode = 8540 Mean reward = -211.87869299880472\n",
            "Episode = 8550 Mean reward = -320.9857948840682\n",
            "Episode = 8560 Mean reward = -138.3409673391111\n",
            "Episode = 8570 Mean reward = -115.8464624253204\n",
            "Episode = 8580 Mean reward = -185.81839865410524\n",
            "Episode = 8590 Mean reward = -137.39290785015007\n",
            "Episode = 8600 Mean reward = -137.8782129414505\n",
            "Episode = 8610 Mean reward = -132.80600796525147\n",
            "Episode = 8620 Mean reward = -200.2455262162959\n",
            "Episode = 8630 Mean reward = -187.03340435107216\n",
            "Episode = 8640 Mean reward = -231.23850286226283\n",
            "Episode = 8650 Mean reward = -117.49495849626604\n",
            "Episode = 8660 Mean reward = -173.33861016979418\n",
            "Episode = 8670 Mean reward = -227.46542357618213\n",
            "Episode = 8680 Mean reward = -112.90906212785615\n",
            "Episode = 8690 Mean reward = -224.00434369363424\n",
            "Episode = 8700 Mean reward = -167.18756892813596\n",
            "Episode = 8710 Mean reward = -171.7458820901561\n",
            "Episode = 8720 Mean reward = -185.9897632227739\n",
            "Episode = 8730 Mean reward = -167.60602491078868\n",
            "Episode = 8740 Mean reward = -97.30427052595645\n",
            "Episode = 8750 Mean reward = -99.87236891971102\n",
            "Episode = 8760 Mean reward = -251.43129584320656\n",
            "Episode = 8770 Mean reward = -87.73533740070152\n",
            "Episode = 8780 Mean reward = -120.72843317272273\n",
            "Episode = 8790 Mean reward = -144.40110467992878\n",
            "Episode = 8800 Mean reward = -136.16308799814288\n",
            "Episode = 8810 Mean reward = -193.94145172880573\n",
            "Episode = 8820 Mean reward = -125.10551408753255\n",
            "Episode = 8830 Mean reward = -171.16713382561224\n",
            "Episode = 8840 Mean reward = -164.1049958397162\n",
            "Episode = 8850 Mean reward = -169.30018709317594\n",
            "Episode = 8860 Mean reward = -141.82774891373205\n",
            "Episode = 8870 Mean reward = -219.15030667513193\n",
            "Episode = 8880 Mean reward = -178.47428931867552\n",
            "Episode = 8890 Mean reward = -163.7885597320728\n",
            "Episode = 8900 Mean reward = -138.26601300422035\n",
            "Episode = 8910 Mean reward = -149.379633528257\n",
            "Episode = 8920 Mean reward = -166.47627170511387\n",
            "Episode = 8930 Mean reward = -209.8912762263481\n",
            "Episode = 8940 Mean reward = -205.4355972845007\n",
            "Episode = 8950 Mean reward = -130.9414221252515\n",
            "Episode = 8960 Mean reward = -151.35223768513066\n",
            "Episode = 8970 Mean reward = -225.4879855144505\n",
            "Episode = 8980 Mean reward = -142.34927169391207\n",
            "Episode = 8990 Mean reward = -140.8051793961501\n",
            "Episode = 9000 Mean reward = -211.56103553326972\n",
            "Episode = 9010 Mean reward = -184.555455590402\n",
            "Episode = 9020 Mean reward = -161.6752456267888\n",
            "Episode = 9030 Mean reward = -173.2146830423572\n",
            "Episode = 9040 Mean reward = -159.45953742845614\n",
            "Episode = 9050 Mean reward = -109.68237171823971\n",
            "Episode = 9060 Mean reward = -173.66981632787224\n",
            "Episode = 9070 Mean reward = -112.1868128058583\n",
            "Episode = 9080 Mean reward = -110.668032647447\n",
            "Episode = 9090 Mean reward = -194.30802118097458\n",
            "Episode = 9100 Mean reward = -146.66153927591293\n",
            "Episode = 9110 Mean reward = -111.03250322018843\n",
            "Episode = 9120 Mean reward = -125.01490068405212\n",
            "Episode = 9130 Mean reward = -172.06938592661248\n",
            "Episode = 9140 Mean reward = -146.31593487093636\n",
            "Episode = 9150 Mean reward = -112.07115919752871\n",
            "Episode = 9160 Mean reward = -159.86572966320963\n",
            "Episode = 9170 Mean reward = -267.936046968575\n",
            "Episode = 9180 Mean reward = -208.22804491820244\n",
            "Episode = 9190 Mean reward = -202.58008894803282\n",
            "Episode = 9200 Mean reward = -244.02115212336895\n",
            "Episode = 9210 Mean reward = -215.32826852926596\n",
            "Episode = 9220 Mean reward = -115.52326113004881\n",
            "Episode = 9230 Mean reward = -205.30772202031858\n",
            "Episode = 9240 Mean reward = -158.3038970779773\n",
            "Episode = 9250 Mean reward = -146.72525359063326\n",
            "Episode = 9260 Mean reward = -128.81816711090852\n",
            "Episode = 9270 Mean reward = -164.54494488668723\n",
            "Episode = 9280 Mean reward = -159.62851345198223\n",
            "Episode = 9290 Mean reward = -186.32134032718358\n",
            "Episode = 9300 Mean reward = -201.3580931376093\n",
            "Episode = 9310 Mean reward = -84.81547093439345\n",
            "Episode = 9320 Mean reward = -158.60575677621082\n",
            "Episode = 9330 Mean reward = -274.0763953328416\n",
            "Episode = 9340 Mean reward = -166.16710057895244\n",
            "Episode = 9350 Mean reward = -171.1860051426701\n",
            "Episode = 9360 Mean reward = -127.94552627869261\n",
            "Episode = 9370 Mean reward = -131.5682293358784\n",
            "Episode = 9380 Mean reward = -160.09632557586656\n",
            "Episode = 9390 Mean reward = -138.04729193140184\n",
            "Episode = 9400 Mean reward = -123.39583387318075\n",
            "Episode = 9410 Mean reward = -108.87833555186691\n",
            "Episode = 9420 Mean reward = -111.72013836906076\n",
            "Episode = 9430 Mean reward = -227.20908033090035\n",
            "Episode = 9440 Mean reward = -113.06402645374058\n",
            "Episode = 9450 Mean reward = -119.6420527183332\n",
            "Episode = 9460 Mean reward = -151.59993583379548\n",
            "Episode = 9470 Mean reward = -125.88295089999767\n",
            "Episode = 9480 Mean reward = -204.21580264160474\n",
            "Episode = 9490 Mean reward = -111.30683357734631\n",
            "Episode = 9500 Mean reward = -102.34532052273887\n",
            "Episode = 9510 Mean reward = -254.11699492309245\n",
            "Episode = 9520 Mean reward = -415.468110232039\n",
            "Episode = 9530 Mean reward = -210.83734705889597\n",
            "Episode = 9540 Mean reward = -365.72791350070133\n",
            "Episode = 9550 Mean reward = -121.75545152463374\n",
            "Episode = 9560 Mean reward = -107.55490989418315\n",
            "Episode = 9570 Mean reward = -111.17911454086452\n",
            "Episode = 9580 Mean reward = -155.99502342504982\n",
            "Episode = 9590 Mean reward = -162.2730555015417\n",
            "Episode = 9600 Mean reward = -171.83884663125772\n",
            "Episode = 9610 Mean reward = -109.3353087268953\n",
            "Episode = 9620 Mean reward = -211.00887058266147\n",
            "Episode = 9630 Mean reward = -184.08921135665446\n",
            "Episode = 9640 Mean reward = -168.925317245598\n",
            "Episode = 9650 Mean reward = -209.3091706647871\n",
            "Episode = 9660 Mean reward = -156.60315798455753\n",
            "Episode = 9670 Mean reward = -256.5772151662376\n",
            "Episode = 9680 Mean reward = -249.10966583663668\n",
            "Episode = 9690 Mean reward = -163.2786883939173\n",
            "Episode = 9700 Mean reward = -206.50734445569793\n",
            "Episode = 9710 Mean reward = -153.8858584016301\n",
            "Episode = 9720 Mean reward = -183.64072981132037\n",
            "Episode = 9730 Mean reward = -228.85743828371446\n",
            "Episode = 9740 Mean reward = -160.8342929613392\n",
            "Episode = 9750 Mean reward = -183.7047787880777\n",
            "Episode = 9760 Mean reward = -196.59581227414262\n",
            "Episode = 9770 Mean reward = -177.67949987572567\n",
            "Episode = 9780 Mean reward = -181.42337644628472\n",
            "Episode = 9790 Mean reward = -178.36831468242573\n",
            "Episode = 9800 Mean reward = -182.35427471752922\n",
            "Episode = 9810 Mean reward = -189.79507580381554\n",
            "Episode = 9820 Mean reward = -120.15289862960583\n",
            "Episode = 9830 Mean reward = -145.15315577535847\n",
            "Episode = 9840 Mean reward = -157.76220151292506\n",
            "Episode = 9850 Mean reward = -156.22020669704952\n",
            "Episode = 9860 Mean reward = -162.92868047307107\n",
            "Episode = 9870 Mean reward = -134.45851150529913\n",
            "Episode = 9880 Mean reward = -212.08866269281233\n",
            "Episode = 9890 Mean reward = -145.79471767704726\n",
            "Episode = 9900 Mean reward = -156.54875359935437\n",
            "Episode = 9910 Mean reward = -136.72642786611877\n",
            "Episode = 9920 Mean reward = -141.77743462329644\n",
            "Episode = 9930 Mean reward = -120.59693755610007\n",
            "Episode = 9940 Mean reward = -167.0194828347944\n",
            "Episode = 9950 Mean reward = -216.40548074276526\n",
            "Episode = 9960 Mean reward = -134.22211497741324\n",
            "Episode = 9970 Mean reward = -183.87089423076347\n",
            "Episode = 9980 Mean reward = -225.71248941190464\n",
            "Episode = 9990 Mean reward = -154.03149116713504\n",
            "Episode = 10000 Mean reward = -144.13129498979313\n",
            "Episode = 10010 Mean reward = -170.7599048197753\n",
            "Episode = 10020 Mean reward = -179.1388074743691\n",
            "Episode = 10030 Mean reward = -128.572980273714\n",
            "Episode = 10040 Mean reward = -159.83992455022374\n",
            "Episode = 10050 Mean reward = -255.39030823894714\n",
            "Episode = 10060 Mean reward = -146.77301879415316\n",
            "Episode = 10070 Mean reward = -110.36769482978075\n",
            "Episode = 10080 Mean reward = -160.8848508594018\n",
            "Episode = 10090 Mean reward = -259.12615811405095\n",
            "Episode = 10100 Mean reward = -259.8391085342743\n",
            "Episode = 10110 Mean reward = -225.312230410145\n",
            "Episode = 10120 Mean reward = -207.9289430571997\n",
            "Episode = 10130 Mean reward = -145.3215376799322\n",
            "Episode = 10140 Mean reward = -204.92163865330804\n",
            "Episode = 10150 Mean reward = -187.859274056743\n",
            "Episode = 10160 Mean reward = -208.63895998697586\n",
            "Episode = 10170 Mean reward = -231.72625219187293\n",
            "Episode = 10180 Mean reward = -258.24059594613516\n",
            "Episode = 10190 Mean reward = -231.52566131475388\n",
            "Episode = 10200 Mean reward = -170.03130304940473\n",
            "Episode = 10210 Mean reward = -238.9380578024727\n",
            "Episode = 10220 Mean reward = -221.44692045284023\n",
            "Episode = 10230 Mean reward = -172.8382829651855\n",
            "Episode = 10240 Mean reward = -173.45815055646617\n",
            "Episode = 10250 Mean reward = -146.41086773213274\n",
            "Episode = 10260 Mean reward = -278.63938772285735\n",
            "Episode = 10270 Mean reward = -183.63419121915024\n",
            "Episode = 10280 Mean reward = -339.3597613081778\n",
            "Episode = 10290 Mean reward = -172.83217501470193\n",
            "Episode = 10300 Mean reward = -269.117772056764\n",
            "Episode = 10310 Mean reward = -234.96673699896854\n",
            "Episode = 10320 Mean reward = -365.2166367457456\n",
            "Episode = 10330 Mean reward = -530.3344930793495\n",
            "Episode = 10340 Mean reward = -119.25840771788214\n",
            "Episode = 10350 Mean reward = -135.0350760423572\n",
            "Episode = 10360 Mean reward = -228.73038319941065\n",
            "Episode = 10370 Mean reward = -392.7357107784268\n",
            "Episode = 10380 Mean reward = -256.2921807085546\n",
            "Episode = 10390 Mean reward = -280.2994141120548\n",
            "Episode = 10400 Mean reward = -179.72787291629663\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}