{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import transform\n",
    "import keras\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "frame = env.reset()\n",
    "\n",
    "# Initialize parameters\n",
    "\n",
    "frame_height = 84\n",
    "frame_width = 84\n",
    "state_size = [frame_height, frame_width, 4]\n",
    "action_size = env.action_space.n\n",
    "learning_rate = 0.000035\n",
    "discount_rate = 0.99\n",
    "\n",
    "eps_start = 1\n",
    "eps_end1 = 0.1\n",
    "eps_end2 = 0.01\n",
    "frame_start1 = 1 * 1000\n",
    "frame_end1 = 50 * 1000\n",
    "\n",
    "batch_size = 32\n",
    "episodes_num = 0\n",
    "max_steps = 0\n",
    "max_frames = 2 * 1000 * 1000\n",
    "max_frames_per_ep = 18 * 1000\n",
    "\n",
    "experience_size = 8 * 1000\n",
    "\n",
    "stack_size = 4\n",
    "\n",
    "no_op_steps = 10\n",
    "\n",
    "update_network_freq = 1000\n",
    "\n",
    "\n",
    "\n",
    "RUN_NUM = 7\n",
    "PATH = 'results'\n",
    "SUMMARIES = 'summaries'\n",
    "RUN_ID = 'run_' + str(RUN_NUM)\n",
    "PATH = os.path.join(PATH, RUN_ID)\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(SUMMARIES, RUN_ID), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space.n)\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(frame):\n",
    "    gray = rgb2gray(frame)\n",
    "    crop = gray[4:-15,5:-5]\n",
    "    norm = crop / 255.0\n",
    "    transformed = transform.resize(norm, [frame_height, frame_width, 1])\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='Qnetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.input = tf.placeholder(dtype=tf.float32, shape=[None, *self.state_size], name='input')\n",
    "            self.target_Q = tf.placeholder(dtype=tf.float32, shape=[None], name='target')\n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(\n",
    "                inputs=self.input, filters=32, kernel_size=8, strides=4,\n",
    "                kernel_initializer=tf.variance_scaling_initializer(scale=2), padding='VALID', \n",
    "                activation=tf.nn.relu, use_bias=False, name='conv1')\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(\n",
    "                inputs=self.conv1, filters=64, kernel_size=4, strides=2,\n",
    "                kernel_initializer=tf.variance_scaling_initializer(scale=2), padding='VALID', \n",
    "                activation=tf.nn.relu, use_bias=False, name='conv2')\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(\n",
    "                inputs=self.conv2, filters=64, kernel_size=3, strides=1,\n",
    "                kernel_initializer=tf.variance_scaling_initializer(scale=2), padding='VALID', \n",
    "                activation=tf.nn.relu, use_bias=False, name='conv3')\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3)\n",
    "            \n",
    "            self.ful_con1 = tf.layers.dense(inputs=self.flatten, units=512, \n",
    "                                    activation=tf.nn.relu, name='ful_con1')\n",
    "            \n",
    "            self.ful_con2 = tf.layers.dense(inputs=self.ful_con1, units=128, \n",
    "                                    activation=tf.nn.relu, name='ful_con2')\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs=self.ful_con2, units=self.action_size, activation=None)\n",
    "            \n",
    "            self.best_action = tf.argmax(self.output, axis=-1)\n",
    "            \n",
    "            self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.predicted_Q = tf.reduce_sum(tf.multiply(self.output, tf.one_hot(self.action, action_size, dtype=tf.float32)), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_Q, predictions=self.predicted_Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    def __init__(self, size, frame_height, frame_width, stack_size, batch_size):\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.stack_size = stack_size\n",
    "        self.batch_size = batch_size\n",
    "        self.now_size = 0\n",
    "        self.now_pos = 0\n",
    "        self.actions = np.zeros(self.size, dtype=np.uint8)\n",
    "        self.frames = np.zeros((self.size, frame_height, frame_width), dtype=np.uint8)\n",
    "        self.rewards = np.zeros(self.size, dtype=np.float32)\n",
    "        self.end_life = np.zeros(self.size, dtype=np.bool)\n",
    "        \n",
    "        self.states = np.zeros((self.batch_size, self.stack_size, frame_height, \n",
    "                                frame_width), dtype=np.uint8)\n",
    "        self.next_states = np.zeros((self.batch_size, self.stack_size, frame_height, \n",
    "                                     frame_width), dtype=np.uint8)\n",
    "        self.indices = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add(self, action, frame, reward, end_life):\n",
    "        self.actions[self.now_pos] = action\n",
    "        self.frames[self.now_pos, ...] = frame\n",
    "        self.rewards[self.now_pos] = reward\n",
    "        self.end_life[self.now_pos] = end_life\n",
    "        self.now_pos = (self.now_pos + 1) % self.size\n",
    "        self.now_size = max(self.now_size, self.now_pos)\n",
    "        \n",
    "    def get_indices(self):\n",
    "        # do not accept frames from different life spans\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = np.random.randint(self.stack_size, self.now_size)\n",
    "                if self.end_life[index - self.stack_size:index].any():\n",
    "                    continue\n",
    "                if index < self.stack_size + 1:\n",
    "                    continue\n",
    "                if index >= self.now_pos and (index - self.stack_size) <= self.now_pos:\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "    \n",
    "    def get_batch(self):\n",
    "        self.get_indices()\n",
    "        for i, j in enumerate(self.indices):\n",
    "            self.states[i] = self.frames[j - 1 - self.stack_size:j - 1, ...]\n",
    "            self.next_states[i] = self.frames[j - self.stack_size:j, ...]\n",
    "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.next_states, axes=(0, 2, 3, 1)), self.end_life[self.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps(count_frames, test):\n",
    "    eps = 0\n",
    "    if test:\n",
    "        eps = 0\n",
    "    elif count_frames < frame_start1:\n",
    "        eps = eps_start\n",
    "    elif count_frames >= frame_start1 and count_frames < frame_end1:\n",
    "        eps = ((count_frames - frame_start1) * (eps_end1 - eps_start)) / (frame_end1 - frame_start1) + eps_start\n",
    "    else:\n",
    "        eps = eps_end1\n",
    "    return eps\n",
    "\n",
    "def get_action(sess, count_frames, state, network, test):\n",
    "    eps = get_eps(count_frames, test)\n",
    "    \n",
    "    if np.random.rand(1) < eps:\n",
    "        return (np.random.randint(0, action_size), eps)\n",
    "    return (sess.run(network.best_action, feed_dict={network.input : \n",
    "                                                    state.reshape((1, *state.shape))}), eps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_reset(env, lives_left, test=False):\n",
    "    frame = env.reset()\n",
    "    lives_left = 0\n",
    "    if_life_lost = True\n",
    "    \n",
    "    if test:\n",
    "        for i in range(np.random.randint(1, no_op_steps)):\n",
    "            frame, _, _, _ = env.step(1)\n",
    "    \n",
    "    processed_frame = process(frame)\n",
    "    state = np.repeat(processed_frame, stack_size, axis=2)\n",
    "    return state, if_life_lost\n",
    "\n",
    "def env_step(env, action, lives_left, state):\n",
    "    next_frame, reward, done, info = env.step(action)\n",
    "    if info['ale.lives'] < lives_left:\n",
    "        if_life_lost = True\n",
    "    else:\n",
    "        if_life_lost = done\n",
    "    lives_left = info['ale.lives']\n",
    "    \n",
    "    processed_next_frame = process(next_frame)\n",
    "    next_state = np.append(state[:, :, 1:], processed_next_frame, axis=2)\n",
    "    return next_state, reward, done, lives_left, if_life_lost, processed_next_frame, next_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network_variables(sess, network_vars, target_network_vars):\n",
    "    assigns = []\n",
    "    for w_agent, w_target in zip(network_vars, target_network_vars):\n",
    "        assigns.append(tf.assign(w_target, w_agent))\n",
    "    \n",
    "    sess.run(assigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('network'):\n",
    "    network = Qnetwork(state_size, action_size, learning_rate)\n",
    "    \n",
    "with tf.variable_scope('target_network'):\n",
    "    target_network = Qnetwork(state_size, action_size, learning_rate)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "WRITER = tf.summary.FileWriter(os.path.join(SUMMARIES, RUN_ID))\n",
    "\n",
    "network_variables = tf.trainable_variables(scope='network')\n",
    "target_network_variables = tf.trainable_variables(scope='target_network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Performance'):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name='loss_summary')\n",
    "    LOSS_SUMMARY = tf.summary.scalar('loss', LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name='reward_summary')\n",
    "    REWARD_SUMMARY = tf.summary.scalar('reward', REWARD_PH)\n",
    "\n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'network/Qnetwork/input' with dtype float and shape [?,84,84,4]\n\t [[node network/Qnetwork/input (defined at <ipython-input-40-93d639bb5ad5>:8)  = Placeholder[dtype=DT_FLOAT, shape=[?,84,84,4], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'network/Qnetwork/input', defined at:\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-45-e5392a98ea73>\", line 4, in <module>\n    network = Qnetwork(state_size, action_size, learning_rate)\n  File \"<ipython-input-40-93d639bb5ad5>\", line 8, in __init__\n    self.input = tf.placeholder(dtype=tf.float32, shape=[None, *self.state_size], name='input')\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5206, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'network/Qnetwork/input' with dtype float and shape [?,84,84,4]\n\t [[node network/Qnetwork/input (defined at <ipython-input-40-93d639bb5ad5>:8)  = Placeholder[dtype=DT_FLOAT, shape=[?,84,84,4], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'network/Qnetwork/input' with dtype float and shape [?,84,84,4]\n\t [[{{node network/Qnetwork/input}} = Placeholder[dtype=DT_FLOAT, shape=[?,84,84,4], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-dad95afacb0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_life\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mq_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mq_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'network/Qnetwork/input' with dtype float and shape [?,84,84,4]\n\t [[node network/Qnetwork/input (defined at <ipython-input-40-93d639bb5ad5>:8)  = Placeholder[dtype=DT_FLOAT, shape=[?,84,84,4], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'network/Qnetwork/input', defined at:\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/artem/anaconda3/envs/rl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-45-e5392a98ea73>\", line 4, in <module>\n    network = Qnetwork(state_size, action_size, learning_rate)\n  File \"<ipython-input-40-93d639bb5ad5>\", line 8, in __init__\n    self.input = tf.placeholder(dtype=tf.float32, shape=[None, *self.state_size], name='input')\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1747, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5206, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/artem/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'network/Qnetwork/input' with dtype float and shape [?,84,84,4]\n\t [[node network/Qnetwork/input (defined at <ipython-input-40-93d639bb5ad5>:8)  = Placeholder[dtype=DT_FLOAT, shape=[?,84,84,4], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "experience = ExperienceBuffer(experience_size, frame_height, frame_width, stack_size, batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    frame_count = 0\n",
    "    episodes_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    while frame_count < max_frames:\n",
    "        episode_reward = 0\n",
    "        lives_left = 0\n",
    "        state, if_life_lost = env_reset(env, lives_left)\n",
    "        for i in range(max_frames_per_ep):\n",
    "            action, eps = get_action(sess, frame_count, state, network, False)\n",
    "            \n",
    "            next_state, reward, done, lives_left, if_life_lost, processed_next_frame, next_frame = env_step(env, action, lives_left, state)\n",
    "            \n",
    "            frame_count += 1\n",
    "            episode_reward += reward\n",
    "            \n",
    "            experience.add(action, processed_next_frame[:, :, 0], reward, if_life_lost)\n",
    "            \n",
    "            # learning\n",
    "            if frame_count % 4 == 0 and frame_count > frame_start1:\n",
    "                states, actions, rewards, next_states, end_life = experience.get_batch()\n",
    "                \n",
    "                q_vals = sess.run(target_network.output, feed_dict = {target_network.input : next_states})\n",
    "                \n",
    "                q_target = []\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    q_target.append(rewards[i])\n",
    "                    if not end_life[i]:\n",
    "                        q_target[i] += discount_rate * np.max(q_vals[i])\n",
    "                \n",
    "                loss, opt = sess.run([network.loss, network.update], \n",
    "                                     feed_dict = {network.input : states, \n",
    "                                                  network.action : actions,\n",
    "                                                  network.target_Q : q_target})\n",
    "                \n",
    "                losses.append(loss)\n",
    "                \n",
    "            # update target network\n",
    "            if frame_count % update_network_freq == 0 and frame_count > frame_start1:\n",
    "                update_network_variables(sess, network_variables, target_network_variables)\n",
    "            \n",
    "            if done:\n",
    "                done = False\n",
    "                break\n",
    "        \n",
    "        episodes_rewards.append(episode_reward)\n",
    "        \n",
    "        if len(episodes_rewards) % 10 == 0:\n",
    "            if frame_count > frame_start1:\n",
    "                summ = sess.run(PERFORMANCE_SUMMARIES, \n",
    "                                feed_dict={LOSS_PH:np.mean(losses), \n",
    "                                           REWARD_PH:np.mean(episodes_rewards[-100:])})\n",
    "                \n",
    "                WRITER.add_summary(summ, frame_count)\n",
    "                \n",
    "                print('Episodes =', len(episodes_rewards), 'Frames =', frame_count, 'Mean reward =', np.mean(episodes_rewards[-100:]), 'Mean loss =', np.mean(losses[-100:]), 'Eps =', eps)\n",
    "            \n",
    "            saver.save(sess, PATH + '/model', global_step=frame_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(frame_count, frames_list, reward, path):\n",
    "    for ind, frame_ind in enumerate(frames_list):\n",
    "        frames_list[ind] = resize(frame_ind, (420, 320, 3), preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{path}{\"ATARI_frame_{0}_reward_{1}.gif\".format(frame_count, reward)}', \n",
    "                    frames_list, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "gif_path = 'GIF/'\n",
    "os.makedirs(gif_path, exist_ok=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(PATH + '/model-447213.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(PATH + '/'))\n",
    "    \n",
    "    frames_list = []\n",
    "    total_reward = 0\n",
    "    lives_left = 0\n",
    "    state, if_life_lost = env_reset(env, lives_left)\n",
    "    \n",
    "    while True:\n",
    "        if if_life_lost:\n",
    "            action = 1\n",
    "        else:\n",
    "            action, _ = get_action(sess, 0, state, network, True)\n",
    "            action = int(action)\n",
    "        print(action, end=' ')\n",
    "        next_state, reward, done, lives_left, if_life_lost, processed_next_frame, next_frame = env_step(env, action, lives_left, state)\n",
    "        total_reward += reward\n",
    "        frames_list.append(next_frame)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    print('Reward:', total_reward)\n",
    "    generate_gif(RUN_NUM, frames_list, total_reward, gif_path)\n",
    "    print('Gif generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
