{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRPO.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "p6W9JdXxKHzR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import gym\n",
        "import scipy.signal # for cumulative returns\n",
        "from numpy.linalg import inv # for conjugate gradient\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_NrB3i-zKQRJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# PARAMETERS\n",
        "\n",
        "env = gym.make('Pendulum-v0')\n",
        "state_size = env.observation_space.shape\n",
        "action_size = env.action_space.shape\n",
        "\n",
        "total_episodes = 30\n",
        "gamma = 0.99\n",
        "max_path_len = 2000\n",
        "total_steps = 20000\n",
        "cg_damping = 0.1\n",
        "max_kl = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lJoWSCLHbQ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# FUNCTIONS\n",
        "\n",
        "def gauss_log_prob(x, mu, logstd):\n",
        "    var = tf.math.exp(2 * logstd)\n",
        "    prob = -tf.math.square(x - mu) / (2 * var) - 0.5 * tf.math.log(tf.constant(2 * np.pi)) - logstd\n",
        "    return tf.math.reduce_sum(prob, axis = 1)\n",
        "\n",
        "\n",
        "def gauss_kl(mu1, logstd1, mu2, logstd2):\n",
        "    var1 = tf.math.exp(2 * logstd1)\n",
        "    var2 = tf.math.exp(2 * logstd2)\n",
        "    return tf.math.reduce_sum(logstd2 - logstd1 + (var1 + tf.math.square(mu1 - mu2)) / (2 * var2) - 0.5)\n",
        "\n",
        "\n",
        "def gauss_entropy(mu, logstd):\n",
        "    return tf.math.reduce_sum(logstd + 0.5 * tf.constant(2 * np.pi * np.e, dtype = tf.float32))\n",
        "\n",
        "\n",
        "def var_shape(x):\n",
        "    return [k.value for k in x.get_shape()]\n",
        "\n",
        "\n",
        "def numel(x):\n",
        "    return np.prod(var_shape(x))\n",
        "\n",
        "\n",
        "def flat_grad(loss, var_list):\n",
        "    grads = tf.gradients(loss, var_list)\n",
        "    return tf.concat(values=[tf.reshape(grad, [numel(v)]) for (v, grad) in zip(var_list, grads)], axis=0)\n",
        "\n",
        "\n",
        "def gauss_kl_firstfixed(mu, logstd):\n",
        "    mu1, logstd1 = map(tf.stop_gradient, [mu, logstd])\n",
        "    \n",
        "    return gauss_kl(mu1, logstd1, mu, logstd)\n",
        "\n",
        "\n",
        "def slice_vector(vector, shapes):\n",
        "    start = 0\n",
        "    tensors = []\n",
        "    for shape in shapes:\n",
        "        size = np.prod(shape)\n",
        "        tensor = tf.reshape(vector[start:(start + size)], shape)\n",
        "        tensors.append(tensor)\n",
        "        start += size\n",
        "    return tensors\n",
        "\n",
        "\n",
        "def get_cumulative_returns(r, gamma):\n",
        "    r = np.array(r)\n",
        "    return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "class DistributionInit:\n",
        "    def __init__(self, filter_mean=True):\n",
        "        self.m1 = 0\n",
        "        self.v = 0\n",
        "        self.n = 0.\n",
        "        self.filter_mean = filter_mean\n",
        "\n",
        "    def __call__(self, o):\n",
        "        self.m1 = self.m1 * (self.n / (self.n + 1)) + o    * 1/(1 + self.n)\n",
        "        self.v = self.v * (self.n / (self.n + 1)) + (o - self.m1)**2 * 1/(1 + self.n)\n",
        "        self.std = (self.v + 1e-6)**.5 # std\n",
        "        self.n += 1\n",
        "        if self.filter_mean: \n",
        "            o1 =  (o - self.m1)/self.std\n",
        "        else:\n",
        "            o1 =  o/self.std\n",
        "        o1 = (o1 > 10) * 10 + (o1 < -10)* (-10) + (o1 < 10) * (o1 > -10) * o1 \n",
        "        return o1\n",
        "\n",
        "\n",
        "dist_init = DistributionInit()\n",
        "\n",
        "\n",
        "def rollout(env, agent, max_path_len, total_steps, gamma):\n",
        "    steps_count = 0\n",
        "    paths = []\n",
        "    \n",
        "    while steps_count < total_steps:\n",
        "        observations, actions, rewards, action_dists_mu, action_dists_logstd = [], [], [], [], []\n",
        "        observation = dist_init(env.reset())\n",
        "        \n",
        "        for _ in range(max_path_len):\n",
        "            steps_count += 1\n",
        "            action, policy = agent.act(observation)\n",
        "            \n",
        "            observations.append(observation)\n",
        "            actions.append(action)\n",
        "            action_dists_mu.append(policy.get('action_dist_mu', []))\n",
        "            action_dists_logstd.append(policy.get('action_dist_logstd', []))\n",
        "            \n",
        "            observation, reward, done, _ = env.step(action)\n",
        "            \n",
        "            observation = dist_init(observation)\n",
        "            rewards.append(reward)\n",
        "            \n",
        "            if done or steps_count == total_steps:\n",
        "                path = {'observations' : np.concatenate(np.expand_dims(observations, axis=0)),\n",
        "                        'action_dists_mu' : np.concatenate(action_dists_mu),\n",
        "                        'action_dists_logstd' : np.concatenate(action_dists_logstd),\n",
        "                        'rewards' : np.array(rewards),\n",
        "                        'actions' :  np.array(actions), \n",
        "                        'cumulative_returns' : get_cumulative_returns(rewards, gamma)\n",
        "                       }\n",
        "                paths.append(path)\n",
        "                break\n",
        "   \n",
        "    return paths\n",
        "\n",
        "\n",
        "class FlatWeights:\n",
        "    def __init__(self, sess, var_list):\n",
        "        self.sess = sess\n",
        "        self.flat = tf.concat([tf.reshape(v, shape=[numel(v)]) for v in var_list], axis=0)\n",
        "        \n",
        "    def __call__(self):\n",
        "        return self.flat.eval(session=self.sess)\n",
        "\n",
        "\n",
        "class LoadFlatWeights:\n",
        "    def __init__(self, sess, var_list):\n",
        "        self.sess = sess\n",
        "        assigns = []\n",
        "        shapes = map(var_shape, var_list)\n",
        "        total_size = sum(np.prod(shape) for shape in shapes)\n",
        "        self.theta = tf.placeholder(dtype=tf.float32, shape=total_size)\n",
        "        \n",
        "        start = 0\n",
        "        for (shape, v) in zip(shapes, var_list):\n",
        "            size = np.prod(shape)\n",
        "            assigns.append(tf.assign(v, tf.reshape(self.theta[start:start + size], shape)))\n",
        "            start += size\n",
        "        \n",
        "        self.op = tf.group(*assigns)\n",
        "        \n",
        "    \n",
        "    def __call__(self, theta):\n",
        "        self.sess.run(self.op, feed_dict={self.theta : theta})\n",
        "\n",
        "    \n",
        "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
        "    p = b.copy()\n",
        "    r = b.copy()\n",
        "    x = np.zeros_like(b)\n",
        "    rdotr = r.dot(r)\n",
        "    for i in range(cg_iters):\n",
        "        z = f_Ax(p)\n",
        "        v = rdotr / (p.dot(z) + 1e-8)\n",
        "        x += v * p\n",
        "        r -= v * z\n",
        "        newrdotr = r.dot(r)\n",
        "        mu = newrdotr / (rdotr + 1e-8)\n",
        "        p = r + mu * p\n",
        "        rdotr = newrdotr\n",
        "        if rdotr < residual_tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "\n",
        "def linesearch(f, x, full_step, expected_improve_ratio):\n",
        "    max_backtracks = 10\n",
        "    accept_ratio = 0.1\n",
        "    loss = f(x)\n",
        "    \n",
        "    for step_frac in (0.5)**(np.arange(max_backtracks)):\n",
        "        new_x = x + step_frac * full_step\n",
        "        new_loss = f(new_x)\n",
        "        actual_improve = loss - new_loss\n",
        "        expected_improve = expected_improve_ratio * step_frac\n",
        "        ratio = actual_improve / expected_improve\n",
        "        \n",
        "        if ratio > accept_ratio and actual_improve > 0:\n",
        "            return new_x\n",
        "        \n",
        "    return x\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-KohT7dxezIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, state_size, action_size, total_episodes, gamma, max_path_len, total_steps, cg_damping=0.1, max_kl=0.01):\n",
        "        self.env = env\n",
        "        self.state_size = state_size[0]\n",
        "        self.action_size = np.prod(action_size).item()\n",
        "        self.sess = tf.Session()\n",
        "        self.total_episodes = total_episodes\n",
        "        self.gamma = gamma\n",
        "        self.max_path_len = max_path_len\n",
        "        self.total_steps = total_steps\n",
        "        self.cg_damping = cg_damping\n",
        "        self.max_kl = max_kl\n",
        "        \n",
        "        self.obs = tf.placeholder(dtype=tf.float32, shape=((None,) + state_size), name='state')\n",
        "        self.action = tf.placeholder(dtype=tf.float32, shape=((None,) + action_size), name='action')\n",
        "        self.advantage = tf.placeholder(dtype=tf.float32, shape=None, name='advantage')\n",
        "        \n",
        "        # Gauss parameters\n",
        "        self.old_action_dist_mu = tf.placeholder(dtype=tf.float32, shape=((None,) + action_size))\n",
        "        self.old_action_dist_logstd = tf.placeholder(dtype=tf.float32, shape=((None,) + action_size))\n",
        "        \n",
        "        # Neural network\n",
        "        self.first_layer = tf.contrib.layers.fully_connected(inputs=self.obs, num_outputs=64, scope='first_layer')\n",
        "        self.second_layer = tf.contrib.layers.fully_connected(inputs=self.first_layer, num_outputs=64, scope='second_layer')\n",
        "        self.output = tf.contrib.layers.fully_connected(inputs=self.second_layer, num_outputs=self.action_size, scope='output')\n",
        "        # output = action_dist_mu\n",
        "        \n",
        "        self.action_dist_logstd_param = tf.Variable((0.01 * np.random.randn(1, self.action_size)).astype(np.float32), name='action_dist_logsd_param')\n",
        "        self.action_dist_logstd = tf.tile(self.action_dist_logstd_param, tf.stack((tf.shape(self.output)[0], 1)))\n",
        "        \n",
        "        log_prob = gauss_log_prob(self.action, self.output, self.action_dist_logstd)\n",
        "        log_prob_old = gauss_log_prob(self.action, self.old_action_dist_mu, self.old_action_dist_logstd)\n",
        "        \n",
        "        # L_surr = -E[ prob / prob_old * advantage ]\n",
        "        frac = tf.exp(log_prob - log_prob_old)\n",
        "        L_surr = -tf.math.reduce_mean(frac * self.advantage)\n",
        "        \n",
        "        var_list = tf.trainable_variables()\n",
        "        \n",
        "        batch_size = tf.cast(tf.shape(self.obs)[0], dtype=tf.float32)\n",
        "        kl = gauss_kl(self.old_action_dist_mu, self.old_action_dist_logstd, self.output, self.action_dist_logstd) / batch_size\n",
        "        entropy = gauss_entropy(self.output, self.action_dist_logstd) / batch_size\n",
        "        \n",
        "        self.losses = [L_surr, kl, entropy]\n",
        "        \n",
        "        self.flat_grad_surr = flat_grad(L_surr, var_list)\n",
        "        \n",
        "        kl_firstfixed = gauss_kl_firstfixed(self.output, self.action_dist_logstd) / batch_size\n",
        "        grads = tf.gradients(kl_firstfixed, var_list)\n",
        "        \n",
        "        self.flat_tangent = tf.placeholder(dtype=tf.float32, shape=None)\n",
        "        shapes = map(var_shape, var_list)\n",
        "        tangents = slice_vector(self.flat_tangent, shapes)\n",
        "        \n",
        "        grad_vec_prod = [tf.math.reduce_sum(g * t) for (g, t) in zip(grads, tangents)]\n",
        "        self.fisher_vec_prod = flat_grad(grad_vec_prod, var_list)\n",
        "        \n",
        "        self.flat_weights = FlatWeights(self.sess, var_list)\n",
        "        self.load_flat_weights = LoadFlatWeights(self.sess, var_list)\n",
        "        self.sess.run(tf.variables_initializer(var_list))\n",
        "        \n",
        "        \n",
        "    def act(self, obs):\n",
        "        obs = np.expand_dims(obs, axis=0)\n",
        "        action_dist_mu, action_dist_logstd = self.sess.run([self.output, self.action_dist_logstd], feed_dict={self.obs : obs})\n",
        "        action = action_dist_mu + np.exp(action_dist_logstd) * np.random.randn(*action_dist_logstd.shape)\n",
        "        \n",
        "        return_dict = {'action_dist_mu' : action_dist_mu, 'action_dist_logstd' : action_dist_logstd}\n",
        "        \n",
        "        return action.ravel(), return_dict\n",
        "        \n",
        "    def learn(self):\n",
        "        episode_count = 0\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i in range(self.total_episodes):\n",
        "            \n",
        "            print (\"\\n********** Iteration %i ************\" % i)\n",
        "            \n",
        "            print('Rollout')\n",
        "            paths = rollout(self.env, self, self.max_path_len, self.total_steps, self.gamma)\n",
        "            print('Made rollout')\n",
        "            \n",
        "            # Compute advantage\n",
        "            for path in paths:\n",
        "                path['advantages'] = path['cumulative_returns']\n",
        "            \n",
        "            # Updating policy\n",
        "            observations = np.concatenate([path['observations'] for path in paths])\n",
        "            actions = np.concatenate([path['actions'] for path in paths])\n",
        "            action_dist_mu = np.concatenate([path['action_dists_mu'] for path in paths])\n",
        "            action_dist_logstd = np.concatenate([path['action_dists_logstd'] for path in paths])\n",
        "            \n",
        "            # Normalizing advantages\n",
        "            advantages = np.concatenate([path['advantages'] for path in paths])\n",
        "            advantages -= advantages.mean()\n",
        "            advantages /= (advantages.std() + 1e-8)\n",
        "            \n",
        "            feed_dict = {self.obs : observations, \n",
        "                            self.action : actions,\n",
        "                            self.advantage : advantages, \n",
        "                            self.old_action_dist_mu : action_dist_mu, \n",
        "                            self.old_action_dist_logstd : action_dist_logstd\n",
        "                        }\n",
        "            \n",
        "            prev_theta = self.flat_weights()\n",
        "            \n",
        "            def fisher_vector_product(p):\n",
        "                feed_dict[self.flat_tangent] = p\n",
        "                return self.sess.run(self.fisher_vec_prod, feed_dict=feed_dict) + p * cg_damping\n",
        "            \n",
        "            flat_grad = self.sess.run(self.flat_grad_surr, feed_dict=feed_dict)\n",
        "            \n",
        "            step_dir = conjugate_gradient(fisher_vector_product, -flat_grad)\n",
        "            \n",
        "            shs = (0.5 * step_dir.dot(fisher_vector_product(step_dir)))\n",
        "            \n",
        "            lm = np.sqrt(shs / self.max_kl)\n",
        "            \n",
        "            full_step = step_dir / lm\n",
        "            \n",
        "            def loss(theta):\n",
        "                self.load_flat_weights(theta)\n",
        "                return self.sess.run(self.losses[0], feed_dict=feed_dict)\n",
        "            \n",
        "            new_theta = linesearch(loss, prev_theta, full_step, -flat_grad.dot(step_dir) / lm)\n",
        "            new_theta = prev_theta + full_step\n",
        "            \n",
        "            self.load_flat_weights(new_theta)\n",
        "            \n",
        "            # Report progress\n",
        "            L_surr, kl, entropy = self.sess.run(self.losses, feed_dict=feed_dict)\n",
        "            episode_rewards = np.array([path['rewards'] for path in paths])\n",
        "            episode_count += len(episode_rewards)\n",
        "            \n",
        "            stats = {}\n",
        "            stats['Total number of episodes'] = episode_count\n",
        "            stats['Average sum of rewards per episode'] = episode_rewards.mean()\n",
        "            stats['Entropy'] = entropy\n",
        "            stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n",
        "            stats['KL between old and new distribution'] = kl\n",
        "            stats['Surrogate loss'] = L_surr\n",
        "            for k, v in stats.items():\n",
        "                print(k + ': ' + ' ' * (40 - len(k)) + str(v))\n",
        "            \n",
        "            if entropy != entropy:\n",
        "                exit(-1)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X_s--95Ur9pM",
        "colab_type": "code",
        "outputId": "89abb108-33c6-4938-9b87-a8f64b05e18d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6819
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "agent = Agent(env, state_size, action_size, total_episodes, gamma, max_path_len, total_steps, cg_damping, max_kl)\n",
        "agent.learn()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "********** Iteration 0 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 0 --------------------\n",
            "Total number of episodes:                 100\n",
            "Average sum of rewards per episode:       -6.360875527495204\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             0.39 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -3.0517577e-09\n",
            "\n",
            "********** Iteration 1 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 1 --------------------\n",
            "Total number of episodes:                 200\n",
            "Average sum of rewards per episode:       -6.118891335012004\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             0.75 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -1.5258789e-09\n",
            "\n",
            "********** Iteration 2 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 2 --------------------\n",
            "Total number of episodes:                 300\n",
            "Average sum of rewards per episode:       -5.875721356725273\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             1.12 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -6.1035155e-09\n",
            "\n",
            "********** Iteration 3 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 3 --------------------\n",
            "Total number of episodes:                 400\n",
            "Average sum of rewards per episode:       -5.857641998791355\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             1.49 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           1.5258789e-09\n",
            "\n",
            "********** Iteration 4 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 4 --------------------\n",
            "Total number of episodes:                 500\n",
            "Average sum of rewards per episode:       -6.065113054477694\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             1.85 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -3.0517577e-09\n",
            "\n",
            "********** Iteration 5 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 5 --------------------\n",
            "Total number of episodes:                 600\n",
            "Average sum of rewards per episode:       -5.772080546828796\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             2.22 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -0.0\n",
            "\n",
            "********** Iteration 6 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 6 --------------------\n",
            "Total number of episodes:                 700\n",
            "Average sum of rewards per episode:       -5.9617389350899535\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             2.59 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -0.0\n",
            "\n",
            "********** Iteration 7 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 7 --------------------\n",
            "Total number of episodes:                 800\n",
            "Average sum of rewards per episode:       -6.172391717108576\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             2.96 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -1.5258789e-09\n",
            "\n",
            "********** Iteration 8 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 8 --------------------\n",
            "Total number of episodes:                 900\n",
            "Average sum of rewards per episode:       -5.969268749335069\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             3.33 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -0.0\n",
            "\n",
            "********** Iteration 9 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 9 --------------------\n",
            "Total number of episodes:                 1000\n",
            "Average sum of rewards per episode:       -6.093735025431788\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             3.70 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -1.5258789e-09\n",
            "\n",
            "********** Iteration 10 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 10 --------------------\n",
            "Total number of episodes:                 1100\n",
            "Average sum of rewards per episode:       -5.898094193054456\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             4.07 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -0.0\n",
            "\n",
            "********** Iteration 11 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 11 --------------------\n",
            "Total number of episodes:                 1200\n",
            "Average sum of rewards per episode:       -5.799292681569218\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             4.44 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -6.1035155e-09\n",
            "\n",
            "********** Iteration 12 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 12 --------------------\n",
            "Total number of episodes:                 1300\n",
            "Average sum of rewards per episode:       -5.978662319941423\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             4.80 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -3.0517577e-09\n",
            "\n",
            "********** Iteration 13 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 13 --------------------\n",
            "Total number of episodes:                 1400\n",
            "Average sum of rewards per episode:       -5.936275820372772\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             5.17 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -3.0517577e-09\n",
            "\n",
            "********** Iteration 14 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 14 --------------------\n",
            "Total number of episodes:                 1500\n",
            "Average sum of rewards per episode:       -5.832588389221295\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             5.54 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           3.0517577e-09\n",
            "\n",
            "********** Iteration 15 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 15 --------------------\n",
            "Total number of episodes:                 1600\n",
            "Average sum of rewards per episode:       -6.0078704258767495\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             5.91 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -1.5258789e-09\n",
            "\n",
            "********** Iteration 16 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 16 --------------------\n",
            "Total number of episodes:                 1700\n",
            "Average sum of rewards per episode:       -6.088913947967913\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             6.28 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           3.0517577e-09\n",
            "\n",
            "********** Iteration 17 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 17 --------------------\n",
            "Total number of episodes:                 1800\n",
            "Average sum of rewards per episode:       -5.950692477094966\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             6.65 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -0.0\n",
            "\n",
            "********** Iteration 18 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 18 --------------------\n",
            "Total number of episodes:                 1900\n",
            "Average sum of rewards per episode:       -5.823684000295947\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             7.02 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -3.8146974e-09\n",
            "\n",
            "********** Iteration 19 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 19 --------------------\n",
            "Total number of episodes:                 2000\n",
            "Average sum of rewards per episode:       -5.778919473614933\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             7.38 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           4.577637e-09\n",
            "\n",
            "********** Iteration 20 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 20 --------------------\n",
            "Total number of episodes:                 2100\n",
            "Average sum of rewards per episode:       -6.203631908328723\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             7.76 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -6.1035155e-09\n",
            "\n",
            "********** Iteration 21 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 21 --------------------\n",
            "Total number of episodes:                 2200\n",
            "Average sum of rewards per episode:       -6.035702021003867\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             8.13 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           1.1444092e-09\n",
            "\n",
            "********** Iteration 22 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 22 --------------------\n",
            "Total number of episodes:                 2300\n",
            "Average sum of rewards per episode:       -5.603568978198078\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             8.51 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           3.0517577e-09\n",
            "\n",
            "********** Iteration 23 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 23 --------------------\n",
            "Total number of episodes:                 2400\n",
            "Average sum of rewards per episode:       -6.049954270449218\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             8.87 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           2.2888185e-09\n",
            "\n",
            "********** Iteration 24 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 24 --------------------\n",
            "Total number of episodes:                 2500\n",
            "Average sum of rewards per episode:       -6.206027936494273\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             9.24 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           4.577637e-09\n",
            "\n",
            "********** Iteration 25 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 25 --------------------\n",
            "Total number of episodes:                 2600\n",
            "Average sum of rewards per episode:       -5.720754744434971\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             9.61 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -1.9073487e-09\n",
            "\n",
            "********** Iteration 26 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 26 --------------------\n",
            "Total number of episodes:                 2700\n",
            "Average sum of rewards per episode:       -6.298815719253483\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             9.97 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -4.577637e-09\n",
            "\n",
            "********** Iteration 27 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 27 --------------------\n",
            "Total number of episodes:                 2800\n",
            "Average sum of rewards per episode:       -6.1578245718687485\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             10.34 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           -1.5258789e-09\n",
            "\n",
            "********** Iteration 28 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 28 --------------------\n",
            "Total number of episodes:                 2900\n",
            "Average sum of rewards per episode:       -5.723123887922703\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             10.70 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           1.5258789e-09\n",
            "\n",
            "********** Iteration 29 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "\n",
            "-------------------- Iteration 29 --------------------\n",
            "Total number of episodes:                 3000\n",
            "Average sum of rewards per episode:       -6.036154198121098\n",
            "Entropy:                                  8.543242\n",
            "Time elapsed:                             11.06 mins\n",
            "KL between old and new distribution:      0.0\n",
            "Surrogate loss:                           4.577637e-09\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}