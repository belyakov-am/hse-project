{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6W9JdXxKHzR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import gym\n",
    "import scipy.signal # for cumulative returns\n",
    "from numpy.linalg import inv # for conjugate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NrB3i-zKQRJ"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.shape\n",
    "\n",
    "eps = 1e-8\n",
    "total_episodes = 30\n",
    "gamma = 0.99\n",
    "max_path_len = 2000\n",
    "total_steps = 20000\n",
    "cg_damping = 0.1\n",
    "max_kl = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lJoWSCLHbQ8"
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def gauss_log_prob(x, mu, logstd):\n",
    "    var = tf.math.exp(2 * logstd)\n",
    "    prob = -tf.math.square(x - mu) / (2 * var) - 0.5 * tf.math.log(tf.constant(2 * np.pi)) - logstd\n",
    "    return tf.math.reduce_sum(prob, axis = 1)\n",
    "\n",
    "\n",
    "def gauss_kl(mu1, logstd1, mu2, logstd2):\n",
    "    var1 = tf.math.exp(2 * logstd1)\n",
    "    var2 = tf.math.exp(2 * logstd2)\n",
    "    return tf.math.reduce_sum(logstd2 - logstd1 + (var1 + tf.math.square(mu1 - mu2)) / (2 * var2) - 0.5)\n",
    "\n",
    "\n",
    "def gauss_entropy(mu, logstd):\n",
    "    return tf.math.reduce_sum(logstd + 0.5 * tf.constant(2 * np.pi * np.e, dtype = tf.float32))\n",
    "\n",
    "\n",
    "def numel(x):\n",
    "    return np.prod(var_shape(x))\n",
    "\n",
    "\n",
    "def flat_grad(loss, var_list):\n",
    "    grads = tf.gradients(loss, var_list)\n",
    "    return tf.concat(values=[tf.reshape(grad, [numel(v)]) for (v, grad) in zip(var_list, grads)], axis=0)\n",
    "\n",
    "\n",
    "def gauss_kl_firstfixed(mu, logstd):\n",
    "    mu1, logstd1 = map(tf.stod_gradient, [mu, logstd])\n",
    "    \n",
    "    return gauss_kl(mu1, logstd1, mu, logstd)\n",
    "\n",
    "\n",
    "def slice_vector(vector, shapes):\n",
    "    start = 0\n",
    "    tensors = []\n",
    "    for shape in shapes:\n",
    "        size = np.prod(shape)\n",
    "        tensor = tf.reshape(vector[start:(start + size)], shape)\n",
    "        tensors.append(tensor)\n",
    "        start += size\n",
    "    return tensors\n",
    "\n",
    "\n",
    "def get_cumulative_returns(r, gamma):\n",
    "    r = np.array(r)\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class DistributionInit:\n",
    "    def __init__(self, filter_mean=True):\n",
    "        self.m1 = 0\n",
    "        self.v = 0\n",
    "        self.n = 0.\n",
    "        self.filter_mean = filter_mean\n",
    "\n",
    "    def __call__(self, o):\n",
    "        self.m1 = self.m1 * (self.n / (self.n + 1)) + o    * 1/(1 + self.n)\n",
    "        self.v = self.v * (self.n / (self.n + 1)) + (o - self.m1)**2 * 1/(1 + self.n)\n",
    "        self.std = (self.v + 1e-6)**.5 # std\n",
    "        self.n += 1\n",
    "        if self.filter_mean: \n",
    "            o1 =  (o - self.m1)/self.std\n",
    "        else:\n",
    "            o1 =  o/self.std\n",
    "        o1 = (o1 > 10) * 10 + (o1 < -10)* (-10) + (o1 < 10) * (o1 > -10) * o1 \n",
    "        return o1\n",
    "\n",
    "\n",
    "dist_init = DistributionInit()\n",
    "\n",
    "\n",
    "def rollout(env, agent, max_path_len, total_steps, gamma):\n",
    "    steps_count = 0\n",
    "    paths = []\n",
    "    \n",
    "    while steps_count < total_steps:\n",
    "        observations, actions, rewards, action_dists_mu, action_dists_logstd = [], [], [], [], []\n",
    "        observation = dist_init(env.reset())\n",
    "        \n",
    "        for _ in range(max_path_len):\n",
    "            steps_count += 1\n",
    "            action, policy = agent.act(observation)\n",
    "            \n",
    "            observations.append(observation)\n",
    "            actions.append(actions)\n",
    "            action_dists_mu.append(policy.get('action_dist_mu', default=[]))\n",
    "            action_dists_logstd.append(policy.get('action_dist_logstd', default=[]))\n",
    "            \n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            \n",
    "            observation = dist_init(observation)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done or steps_count == total_steps:\n",
    "                path = {'observations' : np.concatenate(np.expand_dims(obs, axis=0)),\n",
    "                        'action_dists_mu' : np.concatenate(action_dists_mu),\n",
    "                        'action_dists_logstd' : np.concatenate(action_dists_logstd),\n",
    "                        'rewards' : np.array(rewards),\n",
    "                        'actions' :  np.array(actions), \n",
    "                        'cumulative_returns' : get_cumulative_returns(rewards, gamma)\n",
    "                       }\n",
    "                paths.append(path)\n",
    "                break\n",
    "   \n",
    "    return paths\n",
    "\n",
    "\n",
    "class FlatWeights:\n",
    "    def __init__(self, sess, var_list):\n",
    "        self.sess = sess\n",
    "        self.flat = tf.concat([tf.reshape(v, shape=[numel(v)]) for v in var_list], axis=0)\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.flat.eval(session=self.sess)\n",
    "\n",
    "\n",
    "class LoadFlatWeights:\n",
    "    def __init__(self, sess, var_list):\n",
    "        self.sess = sess\n",
    "        assigns = []\n",
    "        var_shape = [var.value for var in var_list.get_shape()]\n",
    "        shapes = map(var_shape, var_list)\n",
    "        total_size = sum(np.prod(shape) for shape in shapes)\n",
    "        self.theta = tf.placeholder(dtype=tf.float32, shape=[total_size])\n",
    "        \n",
    "        theta = self.weights\n",
    "        start = 0\n",
    "        for (shape, v) in zip(shapes, var_list):\n",
    "            size = np.prod(shape)\n",
    "            assigns.append(tf.assign(v, tf.reshape(theta[start:start + size], shape)))\n",
    "            start += size\n",
    "        \n",
    "        self.op = tf.group(*assigns)\n",
    "        \n",
    "    \n",
    "    def __call__(self, theta):\n",
    "        self.sess.run(self.op, feed_dict={self.theta : theta})\n",
    "\n",
    "    \n",
    "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
    "    p = b.copy()\n",
    "    r = b.copy()\n",
    "    x = np.zeros_like(b)\n",
    "    rdotr = r.dot(r)\n",
    "    for i in range(cg_iters):\n",
    "        z = f_Ax(p)\n",
    "        v = rdotr / (p.dot(z) + 1e-8)\n",
    "        x += v * p\n",
    "        r -= v * z\n",
    "        newrdotr = r.dot(r)\n",
    "        mu = newrdotr / (rdotr + 1e-8)\n",
    "        p = r + mu * p\n",
    "        rdotr = newrdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "\n",
    "def linesearch(f, x, full_step, expected_improve_ratio):\n",
    "    max_backtracks = 10\n",
    "    accept_ratio = 0.1\n",
    "    loss, _, _ = f(x)\n",
    "    \n",
    "    for (_, step_frac) in enumerate((0.5)**(np.arange(max_backtrack))):\n",
    "        new_x = x + step_frac * full_step\n",
    "        new_loss = f(new_x)\n",
    "        actual_improve = loss - new_loss\n",
    "        expected_improve = expected_improve_ratio * step_frac\n",
    "        ratio = actual_improve / expected_improve\n",
    "        \n",
    "        if ratio > accept_ratio and actual_improve > 0:\n",
    "            return new_x\n",
    "        \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KohT7dxezIP"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, state_size, action_size, eps, total_episodes, gamma, max_path_len, total_steps, cg_damping=0.1, max_kl=0.01):\n",
    "        self.env = env\n",
    "        self.state_size = state_size[0]\n",
    "        self.action_size = np.prod(action_size)\n",
    "        self.sess = tf.Session()\n",
    "        self.eps = eps\n",
    "        self.total_episodes = total_episodes\n",
    "        self.gamma = gamma\n",
    "        self.max_path_len = max_path_len\n",
    "        self.total_steps = total_steps\n",
    "        self.cg_damping = cg_damping\n",
    "        self.max_kl = max_kl\n",
    "        \n",
    "        self.obs = tf.placeholder(dtype=tf.float32, shape=[None, state_size], name='state')\n",
    "        self.action = tf.placeholder(dtype=tf.float32, shape=[None, action_size], name='action')\n",
    "        self.advantage = tf.placeholder(dtype=tf.float32, shape=[None], name='advantage')\n",
    "        \n",
    "        # Gauss parameters\n",
    "        self.old_action_dist_mu = tf.placeholder(dtype=tf.float32, shape=[None, action_size])\n",
    "        self.old_action_dist_logstd = tf.placeholder(dtype=tf.float32, shape=[None, action_size])\n",
    "        \n",
    "        # Neural network\n",
    "        self.first_layer = tf.contrib.layers.fully_connected(inputs=self.obs, num_outputs=64, name='first_layer')\n",
    "        self.second_layer = tf.contrib.layers.fully_connected(inputs=self.first_layer, num_outputs=64, name='second_layer')\n",
    "        self.output = tf.contrib.layers.fully_connected(inputs=self.second_layer, num_outputs=self.action_size, name='output')\n",
    "        \n",
    "        self.action_dist_logstd_param = tf.Variable((0.01 * np.random.randn(1, self.action_size)).astype(np.float32), name='action_dist_logsd_param')\n",
    "        self.action_dist_logstd = tf.tile(self.action_dist_logstd_param, tf.stack(tf.shape(self.output)[0], 1))\n",
    "        \n",
    "        log_prob = gauss_log_prob(self.action, self.action_dist_mu, self.action_dist_logstd)\n",
    "        log_prob_old = gauss_log_prob(self.action, self.old_action_dist_mu, self.old_action_dist_logstd)\n",
    "        \n",
    "        # L_surr = -E[ prob / prob_old * advantage ]\n",
    "        frac = tf.exp(log_prob - log_prob_old)\n",
    "        L_surr = -tf.math.reduce_mean(frac * self.advantage)\n",
    "        \n",
    "        var_list = tf.trainable_variables()\n",
    "        \n",
    "        batch_size = tf.cast(tf.shape(self.state_size)[0], dtype=tf.float32)\n",
    "        kl = gauss_kl(self.old_action_dist_mu, self.old_action_dist_logstd, self.output, self.action_dist_logstd) / batch_size\n",
    "        entropy = gauss_entropy(self.output, self.action_dist_logstd) / batch_size\n",
    "        \n",
    "        self.losses = [L_surr, kl, entropy]\n",
    "        \n",
    "        self.flat_grad_surr = flat_grad(L_surr, var_list)\n",
    "        \n",
    "        kl_firstfixed = gauss_kl_firstfidex(self.output, self.action_dist_logstd) / batch_size\n",
    "        grads = tf.gradients(kl_firstfixed, var_list)\n",
    "        \n",
    "        self.flat_tangent = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "        var_shape = [var.value for var in var_list.get_shape()]\n",
    "        shapes = map(var_shape, var_list)\n",
    "        tangents = slice_vector(self.flat_tangent, shapes)\n",
    "        \n",
    "        grad_vec_prod = [tf.math.reduce_sum(g * t) for (g, t) in zip(grads, tangents)]\n",
    "        self.fisher_vec_prod = flat_grad(grad_vec_prod, var_list)\n",
    "        \n",
    "        self.flat_weights = FlatWeights(self.sess, var_list)\n",
    "        self.load_flat_weights = LoadFlatWeights(self.sess, var_list)\n",
    "        self.sess.run(tf.initialize_variables(var_list))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def act(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        action_dist_mu, action_dist_logstd = self.sess.run([self.output, self.action_dist_logstd], feed_dict={self.obs : obs})\n",
    "        action = action_dist_mu + np.exp(action_dist_logstd) * np.random.randn(*action_dist_logstd.shape)\n",
    "        \n",
    "        return action.ravel(), {'action_dist_mu' : action_dist_mu, 'action_dist_logstd' : action_dist_logstd}\n",
    "        \n",
    "        \n",
    "    def learn(self, ):\n",
    "        episode_count = 0\n",
    "        \n",
    "        for i in range(self.total_episodes):\n",
    "            paths = rollout(self.env, self, self.max_path_len, self.total_steps, self.gamma)\n",
    "            \n",
    "            \n",
    "            # TODO: compute advantages\n",
    "            \n",
    "            \n",
    "            # Updating policy\n",
    "            observations = np.concatenate([path['observations'] for path in paths])\n",
    "            actions = np.concatenate([path['actions'] for path in paths])\n",
    "            action_dist_mu = np.concatenate([path['action_dists_mu'] for path in paths])\n",
    "            action_dist_logstd = np.concatenate([path['action_dists_logstd'] for path in paths])\n",
    "            \n",
    "            # Normalizing advantages\n",
    "            advantages = np.concatenate([path['advantages'] for path in paths])\n",
    "            advantages -= advantages.mean()\n",
    "            advantages /= (advantages.std() + 1e-8)\n",
    "            \n",
    "            self.vf.fit(paths)\n",
    "            \n",
    "            feed_dict = {self.obs : observations, \n",
    "                            self.action : actions,\n",
    "                            self.advantage : advantages, \n",
    "                            self.old_action_dist_mu : action_dist_mu, \n",
    "                            self.old_action_dist_logstd : action_dist_logstd\n",
    "                        }\n",
    "            \n",
    "            prev_theta = self.flat_weights()\n",
    "            \n",
    "            def fisher_vector_product(p):\n",
    "                feed_dict[self.flat_tangent] = p\n",
    "                return self.sess.run(self.fisher_vec_prod, feed_dict=feed_dict) + p * cg_damping\n",
    "            \n",
    "            flat_grad = self.sess.run(self.flat_grad_surr, feed_dict=feed_dict)\n",
    "            \n",
    "            step_dir = conjugate_gradient(fisher_vector_product, -flat_grad)\n",
    "            \n",
    "            shs = (0.5 * step_dir.dot(fisher_vector_product(step_dir)))\n",
    "            \n",
    "            lm = np.sqrt(shs / self.max_kl)\n",
    "            \n",
    "            full_step = step_dir / lm\n",
    "            \n",
    "            def loss(theta):\n",
    "                self.load_flat_weights(theta)\n",
    "                return self.sess.run(self.losses[0], feed_dict=feed_dict)\n",
    "            \n",
    "            new_theta = linesearch(loss, prev_theta, full_step, -flat_grad.dot(stepdir) / lm)\n",
    "            new_theta = prev_theta + full_step\n",
    "            \n",
    "            self.load_flat_weights(new_theta)\n",
    "            \n",
    "            # TODO: manage graphs, visualization\n",
    "            \n",
    "            # Report progress\n",
    "            L_surr, kl, entropy = self.sess.run(self.losses, feed_dict=feed_dict)\n",
    "            episode_rewards = np.array([path['rewards'] for path in paths])\n",
    "            episode_count += len(episode_rewards)\n",
    "            \n",
    "            stats = {}\n",
    "            stats['Total number of episodes'] = episode_count\n",
    "            stats['Average sum of rewards per episode'] = episode_rewards.mean()\n",
    "            stats['Entropy'] = entropy\n",
    "            stats['KL between old and new distribution'] = kl\n",
    "            stats['Surrogate loss'] = L_surr\n",
    "            print('\\n-------------------- Iteration {} --------------------'.format(i))\n",
    "            for k, v in stats.iteritems():\n",
    "                print(k + ': ' + ' ' * (40 - len(k)) + str(v))\n",
    "            \n",
    "            if entropy != entropy:\n",
    "                exit(-1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1355
    },
    "colab_type": "code",
    "id": "X_s--95Ur9pM",
    "outputId": "6719f40d-d2cf-4d68-dd46-6ef263505d80"
   },
   "outputs": [],
   "source": [
    "agent = Agent(env, state_size, action_size, eps, total_episodes, gamma, max_path_len, total_steps, cg_damping, max_kl)\n",
    "agent.learn()\n",
    "\n",
    "# TODO: run TRPO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TRPO.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
